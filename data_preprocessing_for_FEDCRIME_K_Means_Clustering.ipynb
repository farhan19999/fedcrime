{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjFiFqquc0wZ"
      },
      "source": [
        " ------------- Last modifying at 15 Jan, 2024 -----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyBzIpiY3gPx"
      },
      "source": [
        "# Unused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoxiMKUZc5ku"
      },
      "source": [
        "# Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTVxUo2hphsH"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SQm3G-C736GH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "root_folder = './'\n",
        "#set hidden state of the lstm\n",
        "hidden_state_of_lstm = 40\n",
        "# learning_rate = 0.0002\n",
        "learning_rate = 0.001\n",
        "server_percentage = 0.5 # percent of data in server\n",
        "# server_percentage_2 = 1.0 # percent of data in server\n",
        "# user_percentage_2 = 1.0\n",
        "isolated_users_percentage = 0.1  # Change this according to your requirement\n",
        "# isolated_users_percentage = 0.0  # Change this according to your requirement\n",
        "overlapping_percentage = 0  # Change this according to your requirement\n",
        "cluster_head_count = 25 #10,20,30,40,50\n",
        "\n",
        "save_file_path = \"k_means_clustered_\" + \"hs_\" + str(hidden_state_of_lstm) + \"_lr_\" + str(int(learning_rate * 10000)) + \"_Sp_\" + str(int(server_percentage * 100))+\"_Iu_\" + str(int(isolated_users_percentage * 100)) + \"_Op_\" + str(int(overlapping_percentage * 100)) + \"_Cl_\" + str(cluster_head_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPv7FAekPGj9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# new_int = np.random.randint(10)\n",
        "# print(new_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_tMwCO8JHsYQ"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhbjeDM1gL6A"
      },
      "source": [
        "# Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALjzw33w3nN3"
      },
      "source": [
        "## Unused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHOZXEZm3reO"
      },
      "source": [
        "## Old Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpguyERYPGj_"
      },
      "source": [
        "### Data Distribution based of Cross Mixation of a certain amount of data inside both the user data and server data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxT0RzxaPGj_"
      },
      "source": [
        "#### Old User data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1v5QPjwi6MT"
      },
      "source": [
        "# Unused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0-7LHB0i-nL"
      },
      "source": [
        "#### Old Server Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2yY10xKPGj_"
      },
      "source": [
        "### Creating testing data as 1st 3 months data and remaining data as training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Co1hOsnPGkA"
      },
      "source": [
        "# New Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWlA57P2PGkA"
      },
      "source": [
        "## Data Distribution based on \"Server data will have certain portion of data from each 77 region\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys92si83PGkA"
      },
      "source": [
        "### first get the entire train portion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V1lFYxA0PGkA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your CSV file into a DataFrame\n",
        "# train_data = pd.read_csv('./training_crime_data_for_last_9_months.csv')\n",
        "train_data = pd.read_csv('./Final_Training_Data_side.csv')\n",
        "\n",
        "# Assuming 'timestamp' is the column name for timestamp and 'neighbourhood_id' is the column for neighbourhood_id\n",
        "train_data['time'] = pd.to_datetime(train_data['time'], format='%Y-%m-%d %H:%M:%S')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now from the train data, give 100 percent data to both server and all the users, then get a certain cluster heads and assign all the crimes to those cluster heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New User Assigment to Crimes Code for New Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "region:  1, crimes :6638, checkins: 14128, uid: 1278\n",
            "region:  2, crimes :3493, checkins: 5256, uid: 578\n",
            "region:  3, crimes :5658, checkins: 39150, uid: 2832\n",
            "region:  4, crimes :18675, checkins: 28063, uid: 2787\n",
            "region:  5, crimes :7730, checkins: 60565, uid: 5127\n",
            "region:  6, crimes :9667, checkins: 7861, uid: 1242\n",
            "region:  7, crimes :9318, checkins: 7000, uid: 1671\n",
            "region:  8, crimes :4516, checkins: 1737, uid: 267\n",
            "region:  9, crimes :8208, checkins: 3468, uid: 396\n",
            "uid assignment for all the 77 regions done,but there are -1 uid also so we have to replace them with those uids which have very low crime incidents\n",
            "--------------- done with replacing -1 with the random uid which has very less data (random from bottom, around 1946 unique uids which has very very less crime data)\n",
            "######### ------------- All Crimes have been assigned with a User ------------- ##################\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Split train data into server and user based on beta percentage for each neighbourhood_id\n",
        "# server_percentage = 0.25 \n",
        "# server_percentage = 0.70 # percent of data in server\n",
        "\n",
        "# Set the seed\n",
        "seed_value = 46\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "#rename the Community Area to neighbourhood_id\n",
        "train_data.rename(columns={\"Community Area\": \"neighbourhood_id\"}, inplace=True)\n",
        "# Group by neighbourhood_id\n",
        "grouped_train_data = train_data.groupby('neighbourhood_id')\n",
        "\n",
        "\n",
        "server_data = pd.DataFrame()  # To store the server data\n",
        "user_data = pd.DataFrame()    # To store the user data\n",
        "\n",
        "# Iterate through each group\n",
        "for group_name, group_data in grouped_train_data:\n",
        "    # Shuffle the rows within each group\n",
        "    shuffled_group_data = group_data.sample(frac=1, random_state=42)  # Set a random_state for reproducibility\n",
        "    \n",
        "    # Split based on beta percentage\n",
        "    server_rows = shuffled_group_data.head(int(server_percentage * len(shuffled_group_data)))\n",
        "    user_rows = shuffled_group_data.tail(len(shuffled_group_data) - len(server_rows))\n",
        "\n",
        "    # Concatenate the server and user data for each group\n",
        "    server_data = pd.concat([server_data, server_rows])\n",
        "    user_data = pd.concat([user_data, user_rows])\n",
        "\n",
        "# Now, 'server_data' and 'user_data' contain the final server and user data respectively\n",
        "# here the server and user data are both \"train\" data which are non overlapping\n",
        "#TODO: we will assign uid to the user_data now\n",
        "CHECKIN_PATH = './checkins_with_neighborhood_id_side.csv'\n",
        "checkins = pd.read_csv(CHECKIN_PATH)\n",
        "checkins.drop(['venueid', 'venue_category', 'count'], axis=1, inplace=True)\n",
        "checkins['utctime'] = pd.to_datetime(checkins['utctime']).dt.tz_localize(None)\n",
        "checkins.rename(columns={\"utctime\": \"time\"}, inplace=True)\n",
        "checkins.sort_values(by=['time'], inplace=True)\n",
        "\n",
        "# client_list_500 = list(range(1, 501))  # Creates a list from 1 to 500 (inclusive)\n",
        "# np.random.shuffle(client_list_500)\n",
        "def assign_uid_to_crime():\n",
        "    for i in range(1,10):\n",
        "        crimes_i = user_data[user_data['neighbourhood_id'] == i]\n",
        "        checkins_i = checkins[checkins['neighbourhood_id'] == i]\n",
        "        print(f\"region:  {i}, crimes :{crimes_i.shape[0]}, checkins: {checkins_i.shape[0]}, uid: {checkins_i['userid'].unique().shape[0]}\")\n",
        "        if(checkins_i['userid'].unique().shape[0] > 0):\n",
        "            # Shuffle the array to ensure randomness\n",
        "            np.random.shuffle(checkins_i['userid'].unique())\n",
        "\n",
        "            # Take the first cluster_head_count values\n",
        "            unique_user_idssss = checkins_i['userid'].unique()\n",
        "            # print('unique user iddss ',len(unique_user_idssss))\n",
        "            actual_unique_user_length = len(unique_user_idssss)\n",
        "            # print(f'len of actual uniq users for ${i} ',actual_unique_user_length)\n",
        "            # selected_uids = set()\n",
        "            index_count = 0\n",
        "            for index, row in crimes_i.iterrows():\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid']))\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid'].unique()[:cluster_head_count]))\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(20))\n",
        "                # Ensure that we loop back to the beginning of the array if we reach its end\n",
        "                uid = int(unique_user_idssss[index_count % actual_unique_user_length])\n",
        "                # selected_uids.add(uid)\n",
        "                user_data.loc[index, 'uid'] = uid\n",
        "                index_count+=1\n",
        "            \n",
        "            # print(f'selected uid for region {i} is ',len(selected_uids))\n",
        "        else :\n",
        "            for index, row in crimes_i.iterrows():\n",
        "                user_data.loc[index, 'uid'] = -1\n",
        "        # unique_user_idssss = client_list_500[:cluster_head_count]\n",
        "        # unique_user_idssss = checkins_i['userid'].unique()[:cluster_head_count]\n",
        "        # actual_unique_user_length = len(unique_user_idssss)\n",
        "        # for index, row in crimes_i.iterrows():\n",
        "        #     uid = int(unique_user_idssss[index % actual_unique_user_length])\n",
        "        #     user_data.loc[index, 'uid'] = uid\n",
        "    print('uid assignment for all the 77 regions done,but there are -1 uid also so we have to replace them with those uids which have very low crime incidents')\n",
        "    uid_row_count = user_data['uid'].value_counts().reset_index()\n",
        "    uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "    # Sort the DataFrame by 'uid'\n",
        "    uid_row_count = uid_row_count.sort_values(by='crime_count')\n",
        "\n",
        "\n",
        "    # Get the top 1946 uid values\n",
        "    top_uid_list = uid_row_count.head(1946)['uid'].tolist()\n",
        "    # print('top uid list ',top_uid_list)\n",
        "\n",
        "    # Read the DataFrame where uid == -1\n",
        "    # df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "\n",
        "    # # Assign a randomly chosen uid from the top 1946 uid list\n",
        "    # df_uid_minus_one['uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "    # df_uid_minus_one['uid'] = df_uid_minus_one['uid'].astype(int)\n",
        "    \n",
        "    # Use .loc to update the DataFrame where uid == -1\n",
        "    df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "    user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "    # user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=False)\n",
        "    user_data['uid'] = user_data['uid'].astype(int)\n",
        "\n",
        "    print('--------------- done with replacing -1 with the random uid which has very less data (random from bottom, around 1946 unique uids which has very very less crime data)')\n",
        "    print(\"######### ------------- All Crimes have been assigned with a User ------------- ##################\")\n",
        "    \n",
        "assign_uid_to_crime()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Now Use K-Means Clustering to cluster these Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0: [456, 872, 605, 2639, 826, 1347, 4659, 625, 2198, 1924, 109, 380, 100, 984, 373, 4602, 943, 1894, 197, 26, 976, 1810, 1105, 1782, 740, 1106, 4137, 837, 683, 246, 2296, 428, 676, 1747, 354, 1234, 574, 75, 866, 983, 16, 972, 181, 52, 494, 519, 2911, 997, 538, 1572, 1149, 2588, 3246, 540, 304, 439, 99, 1678, 4069, 3083, 2325, 399, 765, 1790, 71, 5006, 303, 919, 2938, 320, 339, 505, 4066, 1178, 606, 572, 1027, 797, 6691, 1557, 846, 244, 1172, 315, 307, 1468, 260, 1935, 2834, 2810, 554, 587, 3595, 3684, 937, 143, 669, 1299, 938, 824, 791, 1254, 967, 969, 1388, 2245, 1379, 190, 1629, 2440, 2149, 1282, 1346, 1905, 3975, 660, 2203, 406, 1333, 704, 4142, 5103, 1173, 6163, 3253, 876, 328, 1261, 3352, 1101, 1426, 3032, 3131, 624, 1294, 514, 930, 2727, 440, 1481, 1300, 836, 2916, 4812, 1632, 2584, 4558, 1858, 2091, 1539, 381, 1524, 1820, 2293, 2377, 1703, 5073, 1487, 1516, 2869, 2267, 3039, 3069, 2472, 2981, 6334, 4107, 3266, 4156, 1821, 2745, 2447, 1588, 2580, 2966, 2281, 2000, 6478, 2603, 6665, 1879, 2069, 4999, 1332, 6445, 787, 1402, 3247, 562, 815, 3056, 2054, 956, 3301, 5620, 5260, 6115, 2642, 838, 4235, 2852, 5694, 2638, 2369, 2971, 2230, 5857, 2607, 3546, 3726, 1043, 4002, 4721, 2856, 3082, 2974, 5368, 4026, 4844, 844, 833, 3986, 1327, 4677, 3698, 3264, 4609, 5098, 2651, 4368, 904, 1987, 3248, 6042, 2998, 1551, 3167, 2094, 1099, 4703, 2256, 1113, 500, 3892, 5958, 5572, 3763, 3874, 522, 3578, 4919, 5048, 1104, 6157, 1633, 2976, 2967, 1064, 4816, 1398, 4709, 4185, 1854, 232, 1279, 719, 3071, 376, 2009, 1602, 420, 2594, 3433, 1698, 3210, 4637, 4075, 5654, 2526, 1613, 1958, 2299, 3109, 1258, 1734, 2579, 1684, 1869, 2304, 1534, 6265, 3657, 5004, 3609, 5785, 6266, 861, 2061, 1880, 3252, 5576, 2771, 2330, 2182, 5559, 1605, 2658, 2949, 2930, 4759, 4356, 2393, 2777, 3263, 2812, 2533, 1916, 2014, 6060, 5130, 1812, 5864, 5530, 2251, 5907, 5230, 2024, 1609, 3089, 3338, 3664, 4473, 4859, 3793, 4339, 362, 1672, 1341, 5405, 3143, 3121, 5146, 2037, 3309, 433, 3077, 2802, 2122, 6532, 6603, 4793, 3520, 3123, 6171, 3024, 4704, 3567, 2360, 2355, 2310, 2659, 4207, 5315, 6162, 3970, 5470, 2064, 5306, 3348, 1506, 4603, 4255, 2753, 4204, 2608, 5520, 6553, 2439, 6492, 3553, 4106, 2429, 3009, 4922, 4116, 558, 4973, 3327, 5007, 6149, 5608, 2596, 3573, 3677, 4014, 4941, 4809, 3026, 4110, 2511, 6657, 3466, 5431, 4211, 3816, 2900, 6103, 4607, 1938, 2425, 5206, 4231, 2465, 2451, 5018, 3479, 5119, 5873, 5538, 5627, 4222, 2456, 2720, 4447, 5392, 3949, 2838, 3603, 2775, 2334, 2937, 2994, 4572, 3719, 4652, 5125, 6557, 4994, 6653, 3075, 5528, 6017, 3371, 5912, 5421, 1493, 3636, 2065, 6367, 4326, 4028, 6274, 2216, 3439, 4989, 5277, 3755, 4292, 5001, 3555, 2288, 2185, 2654, 2715, 6056, 6479, 3128, 5020, 4223, 5716, 4907, 2134, 4666, 4492, 4300, 3838, 3468, 6275, 3703, 5815, 5281, 4067, 6482, 6174, 4939, 3844, 6658, 2033, 4725, 3510, 4749, 6679, 5516, 3400, 4845, 6216, 5237, 2732, 6541, 5643, 2433, 6093, 2575, 3708, 6109, 5578, 4760, 3716, 5239, 6185, 5436, 4846, 6147, 1680, 6597, 3554, 5965, 3339, 6072, 1962, 5957, 6187, 1123, 2597, 5305, 5742, 3450, 5272, 5017, 5838, 5998, 4601, 5605, 6201, 4431, 4951, 6604, 4779, 5660, 6363, 5486, 5348, 5501, 3558, 2392, 3518, 6449, 3436, 1459, 2734, 6001, 2825, 5688, 4086, 3540, 4477, 6267, 4341, 5653, 4910, 5275, 6159, 5365, 6550, 4739, 4664, 6155, 4487, 6481, 4104, 1760, 3397, 5179, 5446, 6370, 6382, 5440, 4340, 5524, 5788, 5953, 5876, 6562, 6673, 6525, 5715, 5963, 5764, 6502, 5320, 5531, 5222, 5226, 5093, 6126, 4419, 6003, 4799, 4711, 3861, 4345, 6511, 5618, 4882, 5845, 4418, 2035, 5454, 4887, 4380, 6376, 6307, 6101, 4470, 2466, 5160, 5843, 5255]\n",
            "Cluster 1: [191, 971, 1525, 1499, 1785, 1344, 389, 1928, 3911, 2668, 2983, 2725, 4683, 2675, 2519, 5127]\n",
            "Cluster 2: [1314, 218, 498, 1190, 804, 1385, 4947, 418, 374, 2783, 1549, 54, 1079, 321, 894, 480, 469, 1587, 1137, 410, 448, 119, 1155, 1476, 352, 979, 609, 2213, 922, 577, 1009, 586, 794, 1030, 693, 827, 796, 393, 1496, 1870, 1495, 1087, 2629, 400, 569, 619, 2045, 503, 1505, 735, 1084, 1543, 1051, 3882, 1264, 1471, 742, 3447, 375, 1305, 878, 768, 3492, 902, 158, 1920, 2833, 654, 674, 2917, 3894, 1676, 1537, 703, 1271, 627, 650, 2106, 1382, 534, 1460, 2002, 1690, 4955, 1195, 986, 620, 2558, 906, 1114, 446, 764, 1761, 1313, 691, 2399, 1039, 2112, 845, 2616, 386, 1318, 926, 2452, 512, 1066, 523, 873, 51, 3337, 1185, 684, 1744, 1577, 1355, 124, 2477, 3473, 645, 1884, 4244, 1102, 426, 4821, 1058, 1283, 1666, 1519, 28, 1259, 296, 989, 600, 2444, 313, 501, 289, 3630, 1159, 1956, 5451, 3035, 648, 159, 1377, 1651, 657, 4055, 655, 679, 2850, 985, 5354, 1600, 2509, 1328, 2038, 1233, 4396, 65, 1503, 1358, 5341, 5645, 342, 97, 2717, 610, 670, 975, 1425, 1677, 186, 613, 1238, 1482, 734, 1550, 245, 668, 1769, 1552, 133, 442, 2141, 1353, 1601, 1192, 1708, 623, 1866, 644, 5106, 1569, 1960, 4373, 630, 2151, 602, 324, 1805, 463, 1437, 1520, 3912, 1589, 2004, 1970, 120, 1089, 4153, 1793, 1594, 411, 145, 718, 1383, 2041, 3388, 1334, 709, 2274, 1764, 2612, 1354, 1413, 1730, 1175, 1840, 4555, 2277, 1378, 570, 3190, 2520, 4766, 77, 1145, 291, 1585, 915, 917, 886, 1004, 1200, 3515, 2015, 1835, 954, 2449, 24, 2879, 1954, 1157, 1484, 990, 1052, 1103, 553, 2023, 2555, 1611, 2160, 677, 1478, 961, 949, 711, 1877, 3367, 2750, 2266, 1873, 1221, 2551, 1650, 1138, 4202, 1160, 1913, 1778, 785, 2388, 492, 762, 1421, 870, 1293, 3019, 3519, 2337, 3675, 17, 2446, 3432, 2899, 4025, 493, 1715, 911, 2036, 2683, 3059, 3076, 1849, 2835, 460, 2057, 680, 2255, 2479, 3722, 1742, 3005, 6498, 1501, 2673, 3866, 3097, 6428, 5503, 5236, 3147, 5333, 1947, 3980, 6632, 6581, 1628, 4342, 5932, 6135, 5249, 1048, 4966, 4526, 6647, 5414, 5682, 6331, 2473, 2540, 6688, 1722, 1489, 4451, 5566, 2443, 6278, 6349, 4304, 6561, 5035, 1405, 5656, 6308, 6528, 5638, 4003, 6316, 6672, 4068, 2811, 3621, 4212, 5148, 5687, 5675, 3350, 5943, 2071, 5632, 5631, 5934, 6592, 2970, 1891, 5402, 2218, 3376, 6294, 2752, 4258, 6659, 4427, 3418, 2696, 3428, 2910, 1245, 1442, 2773, 730, 750, 2286, 2282, 1514, 1590, 3819, 1580, 1788, 2345, 1579, 2307, 2491, 608, 2306, 4387, 1036, 1061, 2618, 2806, 2933, 2308, 2008, 168, 1415, 2204, 2189, 1308, 1886, 3656, 2865, 1511, 2516, 2805, 2778, 3576, 1804, 945, 2101, 1158, 6519, 2240, 2121, 848, 1912, 865, 5663, 5479, 1453, 6197, 5327, 2391, 2790, 1885, 6292, 2633, 6223, 5357, 5922, 2371, 6033, 2016, 2320, 4130, 5231, 1316, 6514, 5265, 6272, 5536, 5459, 3610, 2263, 5888, 5418, 4788, 5853, 6663, 4699, 6329, 6534, 5025, 2029, 485, 3687, 1444, 895, 1574, 3474, 4336, 1695, 1368, 3080, 4388, 2031, 4308, 4357, 2242, 1349, 3094, 2418, 3164, 4138, 1725, 1979, 2925, 2733, 2403, 1417, 2684, 1871, 1638, 4046, 3780, 3709, 4287, 4208, 1904, 2148, 2162, 3409, 2207, 2706, 4081, 4237, 2746, 1614, 1512, 4042, 3325, 722, 3577, 2934, 2346, 3934, 3678, 720, 4671, 4731, 2385, 4895, 3288, 1976, 3344, 4953, 3754, 1927, 2559, 3700, 2653, 3626, 521, 3971, 2249, 2461, 3653, 3789, 3950, 4045, 2499, 4520, 1826, 2754, 4291, 4691, 3122, 3445, 2156, 1860, 4111, 1910, 1852, 1545, 2571, 3044, 3822, 2808, 3004, 2358, 3320, 3850, 2861, 4132, 3782, 1423, 1940, 20, 2877, 4950, 2711, 2430, 1768, 2779, 3243, 3231, 2404, 3198, 1497, 3676, 3356, 1794, 2096, 1434, 1667, 2066, 3114, 1973, 3651, 3057, 2818, 3776, 2536, 1948, 3661, 3179, 1673, 3637, 1618, 3063, 3386, 1143, 3212, 1706, 1211, 3627, 3277, 4088, 1717, 2336, 2628, 2103, 4033, 2166, 4118, 2926, 4438, 3666, 4273, 1763, 2695, 2088, 2475, 1851, 3982, 3195, 3104, 4694, 2150, 4457, 2799, 4367, 1082, 1709, 4302, 3673, 2902, 3022, 4236, 4958, 5037, 3139, 4141, 3557, 1733, 4553, 3399, 3028, 3606, 1620, 2529, 2986, 2490, 3217, 3058, 2228, 3185, 2238, 2858, 1758, 2352, 2872, 2991, 3394, 3172, 2367, 3170, 3184, 3119, 3101, 3166, 4038, 3027, 2120, 3012, 3431, 2053, 3098, 2543, 3940, 4850, 3425, 1964, 2990, 4126, 2709, 2814, 2078, 2144, 4143, 1934, 4813, 2376, 2761, 3990, 2531, 3437, 2349, 3839, 3704, 3478, 2671, 292, 1953, 4795, 2743, 2442, 4076, 3580, 2192, 3694, 2593, 3493, 3869, 3454, 2525, 1929, 3563, 2712, 4475, 3639, 1983, 2414, 4993, 3701, 3933, 3778, 4010, 3872, 5099, 4469, 3319, 3565, 4094, 2698, 2719, 4554, 2217, 3088, 4831, 3232, 2468, 3645, 4909, 4625, 4541, 6336, 2354, 2748, 6461, 4437, 6048, 2674, 3870, 5556, 2882, 5810, 6045, 6483, 5652, 6512, 5732, 5438, 6391, 5986, 4722, 4426, 6596, 6574, 6069, 4161, 3930, 3748, 3735, 4833, 4058, 1119, 3989, 3108, 1980, 3079, 2027, 1645, 4456, 2839, 4735, 4082, 5059, 2637, 2600, 2419, 2796, 3528, 4376, 4210, 3743, 3998, 4851, 3384, 3865, 2415, 4429, 4114, 1950, 4884, 4071, 3461, 3459, 2665, 4226, 2871, 4113, 4323, 2624, 3960, 3728, 3671, 4186, 4708, 5075, 3614, 2940, 5097, 4335, 2138, 4535, 3837, 3452, 4164, 3740, 4911, 4436, 2988, 5045, 4392, 3867, 4528, 3522, 4723, 3941, 5026, 4148, 4792, 3299, 2236, 3613, 3182, 2797, 4928, 2849, 2382, 4712, 4701, 1992, 3427, 4693, 3803, 4707, 3752, 4509, 3446, 4942, 3638, 1615, 2545, 3531, 3820, 4446, 4977, 2416, 4534, 4195, 4282, 3574, 3917, 3962, 4906, 3885, 2132, 3568, 3770, 3494, 2708, 5010, 4886, 4784, 2567, 4997, 4839, 4270, 3663, 4743, 4807, 3895, 3646, 3747, 4503, 4324, 4811, 3486, 4863, 3583, 4834, 4220, 3527, 4036, 5005, 5883, 4251, 4860, 4465, 4678, 6466, 3759, 6210, 5116, 5121, 6589, 5396, 4530, 3818, 6690, 5545, 5282, 5384, 5120, 5153, 4568, 5273, 4654, 5544, 4754, 5034, 4667, 6446, 5318, 5500, 6215, 5152, 6655, 5355, 3045, 6119, 5751, 4188, 6340, 3175, 5442, 5145, 4615, 5326, 6462, 6590, 6085, 6419, 5677, 5637, 5601, 4238, 6234, 5449, 5995, 5328, 6667, 6205, 5488, 5745, 6497, 5586, 5390, 5164, 6677, 4482, 5242, 5472, 3875, 4978, 6263, 3691, 4497, 6555, 6180, 6243, 6660, 6000, 5651, 5427, 4765, 6643, 5786, 6245, 3074, 4856, 6345, 5108, 5233, 5999, 5195, 5350, 6317, 5989, 5434, 6450, 6016, 6195, 5450, 5247, 6027, 6137, 6645, 5439, 6276, 6694, 3465, 5155, 5194, 6139, 5300, 6181, 6588, 6178, 5294, 5657, 5818, 4913, 6361, 5994, 5870, 5347, 6302, 5382, 6154, 6337, 5773, 3545, 5072, 4984, 4829, 3390, 5901, 6610, 6012, 4536, 6129, 4184, 5425, 4804, 5268, 5185, 4590, 5299, 6067, 4584, 6475, 5469, 5574, 5594, 5334, 5139, 6470, 5588, 5738, 5388, 5251, 6633, 5878, 6036, 6296, 5555, 6434, 5689, 6469, 5968, 4767, 3602, 6685, 5205, 2946, 5900, 6630, 4963, 5175, 6152, 6456, 5744, 6670, 4449, 5826, 6196, 5529, 4627, 5797, 4047, 5721, 4785, 5512, 3964, 4379, 3852, 5836, 5823, 5607, 2960, 6535, 5746, 4275, 5485, 6513, 5032, 6040, 3648, 2474, 6500, 3635, 4563, 5011, 4328, 5872, 6406, 6026, 5187, 5211, 6591, 5243, 6364, 6689, 4514, 3810, 6144, 6122, 5156, 6471, 4732, 4819, 4979, 3149, 6044, 5295, 5981, 5142, 6050, 4634, 6099, 5979, 6399, 6549, 3377, 6111, 5420, 6250, 4970, 6540, 5679, 6312, 6235, 5947, 4841, 6584, 5540, 3207, 5770, 5642, 5869, 6193, 5703, 5753, 5808, 5984, 6558, 6323, 5782, 5490, 5343, 6405, 5012, 4506, 5816, 5322, 3402, 5602, 4623, 6148, 5886, 4562, 5003, 6654, 6496, 5997, 6009, 6692, 4399, 6330, 6339, 6384, 4552, 5824, 5532, 5095, 5842, 2993, 6680, 6015, 4402, 5356, 5508, 6503, 3014, 4796, 3272, 5061, 4697, 6088, 3297, 5383, 6018, 5971, 6407, 3316, 6529, 6507, 5884, 5640, 5945, 6504, 6342, 6233, 5514, 6551, 4745, 5310, 4583, 5993, 6362, 4747, 4499, 5351, 6165, 5960, 2718, 3758, 5646, 3302, 4276, 3294, 5225, 3408, 5787, 6256, 5483, 4172, 6381, 4019, 4616, 6613, 5340, 5803, 6351, 6038, 6299, 3715, 4772, 6311, 4354, 5879, 6140, 4303, 6053, 5614, 5975, 5474, 6224, 5962, 5111, 5669, 6423, 6073, 5193, 6569, 6661, 6290, 6100, 5844, 5955, 5658, 5232, 6439, 6387, 6493, 6375, 5944, 4366, 5779, 3551, 6356, 5667, 6226, 6599, 5920, 5730, 5593, 5570, 5927, 6335, 5926, 5177, 5665, 6203, 5166, 6560, 5252, 5180, 6676, 5622, 5416, 4936, 5543, 6189, 5115]\n",
            "Cluster 3: [5189, 5604]\n",
            "Cluster 4: [170, 1166, 604, 733, 656, 864, 1242, 98, 329, 759, 379, 388, 126, 789, 0, 281, 255, 868, 998, 82, 1257, 196, 458, 241, 3772, 312, 43, 275, 766, 69, 822, 495, 108, 122, 6, 1044, 29, 3688, 575, 588, 883, 155, 455, 1162, 215, 369, 1974, 267, 875, 325, 12, 486, 280, 421, 31, 346, 394, 980, 370, 333, 843, 1010, 106, 5623, 6431, 518, 447, 2379, 286, 2486, 1286, 89, 299, 1939, 885, 662, 649, 958, 888, 1007, 1008, 510, 925, 702, 2957, 748, 2202, 585, 692, 151, 42, 3413, 1500, 896, 6559, 6509, 322, 5630, 631, 2493, 628, 256, 323, 157, 2736, 652, 913, 696, 663, 529, 698, 37, 3178, 773, 555, 4538, 2678, 367, 2050, 6437, 415, 444, 880, 1013, 583, 40, 3832, 811, 5176, 107, 746, 2019, 2046, 3053, 235, 70, 887, 2621, 3582, 2159, 2844, 1387, 4, 1452, 2003, 2661, 1371, 408, 445, 658, 330, 345, 451, 193, 4472, 1345, 4805, 4285, 416, 121, 3120, 2013, 390, 2175, 2338, 1510, 3157, 1474, 1570, 790, 2590, 2007, 3029, 4267, 2241, 4144, 2186, 3174, 4676, 1917, 5401, 3048, 2554, 3158, 2205, 2749, 828, 4591, 4234, 4696, 3443, 1967, 1777, 5329, 2222, 1203, 4315, 4539, 3081, 3993, 5596, 2111, 1237, 3405, 3736, 4035, 2866, 1107, 2030, 1712, 3547, 1919, 2104, 1735, 4665, 2883, 1608, 3529, 2837, 6327, 2397, 5676, 1376, 2363, 5881, 4266, 1393, 3211, 3220, 2950, 2941, 6032, 3695, 4233, 263, 1883, 4440, 4073, 3169, 4459, 4124, 3937, 1144, 3513, 2606, 4639, 2208, 1972, 2951, 2527, 3383, 2105, 5906, 2845, 568, 1576, 4599, 1899, 1813, 3173, 1054, 4612, 2146, 4454, 1306, 4969, 3750, 1856, 482, 3517, 830, 2874, 1270, 1624, 3442, 1564, 3280, 1529, 2387, 2350, 3368, 4501, 1890, 4246, 1527, 1445, 2450, 2864, 3330, 4479, 2423, 2100, 3480, 3354, 3824, 4434, 1610, 1338, 1670, 2427, 1923, 1909, 4669, 3091, 2300, 1872, 4658, 1749, 2903, 6520, 3472, 1688, 3021, 2568, 3116, 1766, 2199, 928, 1932, 2021, 1301, 2566, 2701, 3403, 3652, 4209, 2098, 3541, 2726, 2724, 3357, 3985, 3334, 3161, 3271, 884, 3138, 6338, 2822, 5571, 2660, 2610, 3463, 4089, 2791, 556, 1207, 4413, 4203, 2285, 1055, 4286, 3476, 3374, 4363, 4675, 1951, 1988, 2813, 4031, 4310, 3896, 5722, 4253, 2247, 3216, 4091, 6220, 4849, 2666, 2943, 4959, 5931, 4560, 6460, 3807, 892, 3742, 3326, 4674, 4925, 3731, 3055, 5719, 2611, 5554, 3100, 4053, 3225, 3497, 2453, 3961, 3295, 2201, 5244, 3329, 3853, 4641, 6516, 2626, 2398, 4232, 3011, 4864, 4393, 5250, 5771, 5942, 4079, 5951, 4062, 4621, 1679, 5885, 4826, 6524, 5761, 2326, 5056, 3410, 5859, 4586, 3902, 3539, 3854, 4548, 3548, 4550, 4318, 3534, 3127, 2502, 2714, 5404, 4277, 6004, 6002, 1784, 6455, 3392, 4798, 2907, 3423, 4050, 2953, 2504, 4523, 3086, 507, 5757, 2884, 3834, 2631, 4196, 4835, 3102, 4500, 6013, 3922, 2939, 6041, 5889, 5091, 3013, 3532, 3370, 5415, 4265, 4802, 2769, 3125, 311, 5800, 3891, 5110, 2329, 6624, 4903, 6106, 2546, 3686, 3298, 2962, 3324, 6618, 4051, 2768, 5892, 1827, 6225, 3509, 2380, 6392, 2880, 5219, 5245, 6055, 3590, 2676, 5693, 6283, 5702, 5940, 3903, 780, 2826, 6578, 2459, 2389, 4375, 4948, 4647, 5494, 5291, 3422, 5344, 3213, 4015, 4078, 4136, 3988, 4825, 5858, 4409, 3222, 6650, 1977, 6300, 3150, 6467, 4117, 1136, 3936, 2564, 6086, 4269, 4777, 6404, 5467, 5874, 2323, 5964, 5739, 5537, 6473, 6251, 6620, 4645, 1596, 5733, 5991, 5504, 3395, 1530, 4908, 5899, 3496, 5208, 6252, 6306, 6485, 4060, 3804, 4576, 6609, 5519, 5564, 4764, 3921, 2947, 3974, 4556, 5891, 4519, 2816, 4453, 5969, 6262, 5502, 3801, 4490, 4657, 4581, 5928, 6318, 5493, 2353, 6309, 4441, 6304, 5579, 6373, 5150, 6108, 5078, 5217, 5625, 4101, 5758, 6476, 5057, 5841, 4383, 4547, 6521, 4174, 6023, 3490, 5680, 3440, 5696, 5856, 5335, 3729, 4416, 5974, 4313, 5168, 6222, 4714, 5203, 6128, 3502, 6110, 5956, 4976, 5737, 5433, 4362, 6429, 3054, 4830, 1360, 3130, 5445, 5866, 6094, 6271, 6237, 3649, 5518, 4178, 5587, 5311, 6389, 4009, 3260, 4301, 5791, 3040, 4791, 5263, 5228, 5557, 6543, 6075, 3774, 6117, 5731, 5270, 2455, 5649, 6646, 4498, 5455, 5399, 3790, 4574, 4570, 5235]\n",
            "Cluster 5: [5610, 2209]\n",
            "Cluster 6: [594, 1074]\n",
            "Cluster 7: [277, 461, 1023, 1915, 344, 2426, 1548, 834, 2056, 6209, 4617]\n",
            "Cluster 8: [479, 67, 626, 2524, 41, 710, 753, 187, 7, 314, 385, 204, 678, 467, 343, 1041, 2063, 327, 806, 1252, 1217, 1093, 1006, 336, 1177, 318, 763, 688, 63, 32, 95, 317, 356, 371, 136, 1652, 93, 1888, 383, 138, 478, 288, 264, 2119, 163, 1164, 459, 1612, 914, 5238, 1411, 3412, 242, 751, 219, 496, 177, 1063, 3909, 6390, 10, 1182, 578, 545, 927, 50, 1403, 19, 857, 1224, 1364, 809, 547, 1240, 1181, 2040, 398, 1109, 5366, 1366, 1961, 1128, 1373, 802, 149, 799, 1710, 792, 474, 4836, 254, 1689, 739, 3191, 934, 546, 3969, 1892, 634, 723, 146, 6675, 4642, 1372, 963, 160, 3963, 760, 212, 2032, 508, 2542, 5287, 935, 238, 1115, 2135, 1867, 2317, 2859, 2243, 154, 1167, 1757, 1630, 616, 1409, 1401, 3070, 1422, 1414, 2253, 2070, 4720, 756, 2647, 3025, 2617, 6286, 3958, 5068, 3300, 2124, 3449, 3995, 2301, 5560, 3884, 6386, 4559, 2049, 3146, 6454, 4828, 2694, 1641, 2093, 1343, 2832, 4316, 1683, 6637, 1491, 1903, 3230, 3278, 1783, 3724, 2997, 5376, 1607, 2598, 3612, 1148, 3385, 2648, 781, 2574, 2072, 5795, 4544, 131, 3887, 4933, 1208, 2977, 4540, 1655, 3965, 2152, 1429, 3591, 2705, 2821, 3484, 430, 3181, 3823, 1198, 2744, 6354, 1199, 1990, 1454, 2912, 1566, 3659, 1163, 2890, 335, 3775, 6153, 5819, 1012, 2372, 1829, 4450, 4815, 1071, 2147, 3491, 1556, 5776, 3761, 2254, 820, 2407, 5671, 1847, 4588, 2562, 3398, 5021, 3152, 1016, 4546, 4484, 319, 6614, 2601, 55, 3415, 2068, 361, 2614, 772, 3353, 1662, 1267, 2512, 2181, 2686, 4872, 1563, 2488, 1419, 3420, 1188, 3769, 1845, 2886, 4351, 5822, 1875, 2184, 3460, 4162, 5000, 6142, 2460, 4773, 3085, 2713, 4800, 5022, 1807, 2929, 1850, 3739, 2174, 1942, 1861, 6518, 5829, 4604, 2275, 5248, 3918, 3629, 3036, 1631, 5915, 4129, 3001, 4752, 4808, 2682, 2237, 6024, 3481, 6095, 1874, 5628, 4333, 5807, 2924, 5298, 3365, 5410, 6202, 2739, 3597, 1775, 4593, 4061, 2324, 2830, 2836, 1723, 3699, 4043, 6395, 5330, 4105, 4189, 4878, 6268, 5338, 5204, 2662, 3906, 2846, 6397, 4713, 2085, 2772, 6105, 6172, 2012, 4706, 2927, 1831, 4123, 5052, 2787, 2233, 3857, 3900, 5584, 2535, 2086, 5381, 5292, 3002, 4290, 3764, 4155, 4874, 2295, 5428, 4630, 6025, 4327, 3453, 5019, 4228, 6083, 3471, 4896, 4943, 5227, 4214, 4729, 2521, 3099, 6010, 5780, 5417, 3000, 2142, 6403, 6606, 4343, 3814, 5186, 6662, 6625, 4128, 2126, 4854, 4606, 3417, 3363, 5084, 3923, 941, 3797, 5615, 2995, 2153, 2679, 5349, 2092, 2956, 2235, 5284, 3813, 6206, 5465, 3631, 5487, 5982, 4063, 4397, 5996, 4466, 3712, 3811, 5359, 5710, 6259, 2137, 4912, 4571, 5616, 4281, 5505, 3593, 5324, 2168, 5361, 3868, 5492, 2699, 3543, 6011, 4838, 2374, 3411, 4929, 4761, 1817, 3942, 4628, 6230, 3953, 6582, 5851, 5976, 4679, 3713, 6517, 6047, 3290, 2517, 3706, 5552, 1702, 6629, 4298, 5279, 3360, 4972, 4192, 3228, 2179, 5317, 6102, 6408, 6332, 2194, 3829, 6077, 3762, 6343, 4670, 4125, 4992, 2862, 5070, 4435, 4289, 2893, 6648, 5595, 4056, 5903, 6168, 4682, 4888, 4384, 4371, 5893, 6448, 4537, 3564, 5636, 2348, 4098, 6297, 4684, 5367, 6232, 5849, 5162, 3051, 5029, 6065, 6575, 4726, 4620, 3569, 4879, 3791, 4417, 6418, 5923, 5541, 2982, 5678, 5413, 6566, 5523, 5569, 5936, 5548, 6063, 4432, 2721, 5165, 5720, 5435, 3556, 4243, 6411, 5374, 3611, 4401, 2484, 4391, 4299, 5443, 5756, 4728, 4095, 5827, 3159, 5319, 6402, 5215, 5894, 4215, 5448, 5729, 5724, 5868, 3444, 4385, 5077, 5102, 5473, 5527, 4738, 4990, 2390, 4513, 2831, 5301, 6576, 6619, 6487, 6623, 6282, 5458, 4012, 6244, 3206, 4250, 3685, 3501, 4491, 6687, 4847, 5992, 6087, 5460]\n",
            "Cluster 9: [849, 338, 96, 1065, 1096, 234, 402, 2193, 1110, 1091, 203, 640, 697, 1024, 665, 537, 179, 1554, 2891, 607, 72, 180, 829, 686, 359, 236, 357, 1152, 2318, 223, 472, 1621, 584, 1253, 1473, 208, 4750, 74, 2687, 778, 1726, 473, 1701, 1098, 231, 732, 47, 391, 1863, 4649, 689, 457, 225, 1197, 854, 964, 25, 695, 6293, 2560, 4152, 1019, 1808, 165, 1369, 2172, 2084, 1475, 350, 1312, 1773, 524, 3538, 111, 939, 83, 22, 174, 701, 1026, 4605, 2224, 823, 1319, 595, 1741, 449, 340, 2123, 1367, 643, 891, 946, 5476, 68, 477, 1025, 162, 2215, 3537, 4508, 2339, 2018, 629, 4869, 3624, 1480, 4510, 253, 4494, 1823, 2343, 5014, 3846, 1791, 443, 4881, 4321, 1848, 1834, 1526, 4579, 6127, 1463, 3284, 2252, 5924, 5313, 2518, 3880, 1914, 5304, 831, 4176, 3904, 5080, 3533, 2782, 2273, 2128, 968, 1213, 2789, 4004, 3214, 2110, 1014, 2171, 5798, 3873, 2514, 2173, 4173, 1597, 3233, 673, 358, 1704, 1721, 5700, 681, 3171, 1966, 2259, 2764, 6315, 5670, 4360, 2448, 3674, 4904, 4146, 1290, 2028, 1165, 4587, 1262, 1302, 3559, 2539, 2176, 2909, 90, 1752, 3379, 1724, 4032, 2457, 6451, 4077, 3269, 6674, 4724, 6360, 3296, 1809, 1443, 4430, 3741, 3886, 1696, 2689, 3821, 2373, 1504, 3049, 1210, 3487, 1975, 2604, 5197, 1779, 2220, 1795, 1816, 1518, 4080, 2738, 1418, 962, 3572, 6414, 6208, 4848, 3134, 2129, 1933, 3285, 5437, 2401, 3925, 1959, 188, 3306, 1642, 1937, 5283, 3469, 2650, 1604, 4365, 1412, 4249, 4157, 4668, 5049, 3200, 3919, 2114, 3333, 3620, 5090, 1357, 2155, 6410, 2922, 3702, 3148, 4041, 3544, 384, 6457, 5171, 2080, 3670, 3858, 5510, 4843, 6480, 4852, 4093, 3655, 3335, 2747, 5784, 3689, 6465, 4404, 3160, 5980, 4758, 2742, 3006, 6556, 2692, 5033, 4940, 3926, 5681, 5008, 4108, 2381, 5266, 5400, 1830, 4408, 5709, 3124, 4334, 2878, 5583, 4261, 5398, 1462, 2342, 3018, 2089, 2272, 3863, 4733, 2780, 4248, 3588, 6420, 2827, 3223, 4421, 6057, 5946, 3788, 4885, 3871, 4295, 4718, 4525, 6444, 2945, 3690, 3137, 6547, 2020, 860, 2841, 3604, 4985, 2544, 1968, 3521, 1969, 3883, 3893, 1243, 3723, 3812, 4359, 1943, 5929, 6695, 4390, 4727, 5452, 6401, 4954, 4507, 4171, 2672, 4406, 5426, 1450, 6198, 2989, 2405, 6179, 2892, 228, 5726, 207, 6398, 4995, 3369, 4005, 5526, 5970, 4150, 4259, 3878, 2079, 3183, 2755, 2506, 2948, 5871, 5131, 6491, 4496, 5027, 6211, 4716, 6133, 3336, 6438, 5167, 5755, 4458, 2760, 4358, 6488, 5987, 5698, 5692, 2495, 5939, 5765, 6593, 5880, 5112, 5828, 5143, 6622, 6059, 5353, 6416, 4915, 3714, 6254, 5128, 4306, 2663, 3115, 6435, 6177, 4975, 5471, 6634, 3738, 2975, 3935, 3387, 6020, 5983, 6383, 6615, 4521, 5264, 4870, 3575, 6192, 5161, 4719, 3827, 5441, 6628, 6577, 1304, 4774, 4423, 4600, 6587, 5598, 5648, 2756, 5735, 5342, 5774, 6393, 4013, 6051, 6350, 2206, 5466, 5229, 6078, 5234, 6156, 5240, 5704, 6141, 4986, 6636, 6160, 5124, 6247, 4710, 6090, 6626, 5429, 6151, 5553, 4347, 5174, 6490, 6249, 6681, 2532, 3992, 3836, 2042, 6463, 4614, 6170, 5727, 4575, 4672, 3650, 5985, 4330, 6501, 5589]\n",
            "Cluster 10: [1100, 135, 1356, 726, 1449, 57, 541, 1035, 64, 1456, 92, 1653, 33, 1130, 470, 227, 38, 699, 566, 527, 1032, 898, 1225, 1174, 73, 770, 409, 434, 525, 767, 835, 3401, 1575, 49, 897, 185, 1311, 899, 173, 700, 355, 1400, 2537, 88, 268, 6348, 1636, 994, 707, 3153, 413, 396, 8, 1281, 195, 1273, 544, 2556, 611, 454, 3618, 871, 1057, 194, 2697, 996, 1214, 576, 784, 4633, 509, 970, 483, 1446, 1176, 2980, 2842, 1649, 1921, 716, 1803, 1406, 407, 114, 1068, 282, 2515, 206, 14, 397, 4935, 499, 58, 3737, 974, 1040, 4549, 1428, 706, 1492, 992, 1231, 372, 1309, 4917, 1352, 755, 1498, 491, 581, 316, 3154, 2322, 671, 2246, 9, 1802, 5407, 1907, 1774, 3815, 2386, 2904, 1786, 199, 2649, 6684, 2437, 3451, 814, 758, 1780, 1404, 1246, 1694, 2409, 2875, 2815, 2200, 1389, 1340, 2481, 1668, 4763, 5092, 4177, 1335, 924, 1303, 4200, 3177, 4478, 1046, 4443, 2411, 2226, 2470, 2462, 4698, 2501, 3720, 3188, 1086, 1776, 1256, 6107, 2547, 2102, 2577, 936, 2581, 4206, 1083, 1464, 1117, 1235, 5551, 5412, 5055, 4159, 3924, 3259, 4594, 2503, 3835, 4971, 5393, 2250, 3619, 5821, 3647, 128, 2914, 2935, 4650, 2026, 4398, 3859, 3007, 1351, 3221, 2225, 5044, 3110, 2043, 4597, 1483, 795, 1647, 3017, 2344, 3256, 2801, 1390, 2116, 2303, 1573, 1171, 6366, 3118, 1220, 2422, 2919, 3314, 2978, 4074, 2234, 5600, 1759, 4967, 1448, 224, 1502, 1765, 3766, 2860, 5043, 2232, 2109, 1737, 6161, 2434, 4880, 1994, 2395, 4011, 1671, 214, 3267, 2716, 3234, 3358, 2557, 4034, 2873, 2652, 3254, 5067, 2309, 2289, 1728, 2931, 1515, 2702, 3607, 1581, 2190, 636, 1941, 2476, 2897, 4573, 2728, 1989, 3020, 3483, 2075, 3187, 3156, 4876, 2487, 3244, 3938, 2881, 2792, 1878, 1900, 1825, 2321, 5606, 2188, 4049, 2313, 4024, 2563, 2615, 1146, 5071, 1857, 4827, 2693, 3457, 1740, 4027, 2276, 1479, 4578, 4566, 4412, 6324, 4898, 6355, 948, 3856, 2406, 1465, 3163, 3598, 6537, 4197, 5289, 3448, 4297, 6594, 3456, 1971, 3809, 4565, 4700, 6380, 1693, 4998, 2312, 1555, 2315, 1528, 5509, 3072, 6656, 1375, 4686, 3802, 3197, 3771, 5909, 3920, 4945, 4983, 5760, 3495, 6595, 3229, 4753, 1833, 2885, 3037, 1743, 1859, 3308, 3323, 3601, 3781, 2992, 6353, 5147, 3250, 3876, 4166, 6158, 3830, 6602, 4121, 6523, 5269, 4096, 3155, 4543, 1477, 4524, 3692, 3947, 3391, 6081, 5713, 1814, 2785, 3842, 3257, 3711, 3414, 6639, 3511, 4252, 3141, 3307, 4893, 4914, 4865, 6064, 5950, 3792, 4424, 3623, 2550, 1161, 3931, 4564, 2851, 4787, 5076, 2485, 4680, 6572, 2513, 2341, 6240, 2108, 2361, 2478, 3090, 3276, 2915, 3783, 3332, 4294, 5789, 5542, 5123, 6379, 4545, 3255, 4160, 6425, 2905, 2857, 1896, 2383, 5549, 3594, 5558, 5307, 4147, 5184, 2793, 4557, 3194, 6199, 3193, 6385, 3050, 3897, 3681, 2681, 3318, 3732, 6298, 2311, 5867, 4871, 6314, 6671, 5419, 4242, 4329, 4122, 6666, 5497, 5748, 3756, 5157, 5847, 4737, 5053, 3224, 3140, 3381, 2505, 3117, 5100, 5159, 6447, 5723, 4640, 6231, 5741, 5074, 6184, 6319, 5060, 6136, 4317, 6538, 4370, 5590, 6388, 5592, 3981, 5387, 5323, 6124, 6347, 1541, 5830, 4394, 5863, 3549, 3095, 5718, 6175, 4891, 4920, 5453, 5895, 5711, 5286, 6029, 4187, 6539, 5262, 5935, 6368, 6357, 5778, 4213, 3043, 5391, 5081, 5611, 5978, 5972, 4769, 6173, 3592, 6621, 6098, 5066, 2921, 5477, 4481, 6531, 5547, 6130, 5395, 6028, 6217, 6289, 6583, 5747, 5385, 4916, 5296, 5647, 6070, 5938, 4488, 5949, 4655, 5444, 6617, 6651, 4349, 4017, 4644, 5096, 3505, 4072, 4527, 6325, 3959, 2420, 3010, 3600, 6505, 6074, 6229, 6097]\n",
            "Cluster 11: [530, 808, 852, 1622, 497, 192, 297, 1720, 2497, 807, 348, 2969, 1681, 592, 141, 294, 130, 737, 2187, 775, 724, 646, 290, 1315, 102, 425, 1011, 39, 918, 729, 788, 2987, 2170, 1140, 1546, 953, 2800, 777, 404, 1811, 271, 6600, 1049, 441, 46, 661, 1841, 539, 3828, 5105, 1122, 1249, 3833, 1324, 1719, 601, 250, 1005, 842, 502, 561, 1955, 5423, 727, 6365, 1634, 5069, 5207, 1895, 1440, 2044, 1292, 851, 1317, 921, 1288, 2165, 1532, 1153, 269, 1180, 1111, 3977, 1553, 347, 437, 2435, 867, 1700, 1018, 184, 4690, 793, 1268, 1077, 3808, 542, 2840, 659, 3951, 959, 995, 618, 2283, 526, 1692, 800, 3236, 3389, 2534, 2287, 1407, 774, 1819, 3928, 2136, 2632, 1447, 2131, 1984, 5038, 1435, 4296, 3665, 4468, 2730, 6544, 5303, 2944, 3142, 1985, 598, 2964, 3189, 1771, 2894, 2055, 3785, 4982, 4322, 1533, 2378, 909, 3215, 171, 5308, 2757, 1957, 3475, 1918, 1981, 2396, 3199, 2942, 6176, 4307, 5363, 1669, 3679, 405, 2180, 3421, 3364, 2006, 5988, 3879, 5754, 2644, 1392, 1792, 2548, 2258, 4853, 2177, 3129, 1120, 1536, 933, 2729, 3396, 4946, 5191, 3347, 5914, 4372, 4403, 3668, 2549, 4486, 61, 1663, 5241, 3322, 2498, 1893, 3241, 4320, 2707, 2959, 3632, 4319, 3132, 2047, 3078, 5086, 6464, 1466, 2809, 2370, 2496, 3718, 940, 3898, 3341, 3499, 1762, 1753, 999, 614, 3287, 2279, 4988, 6494, 3023, 2284, 1798, 5613, 2351, 4087, 6421, 532, 4382, 1339, 1846, 3860, 1623, 4505, 2261, 2963, 3641, 2870, 2786, 2627, 4225, 2863, 6079, 5190, 4154, 4355, 4389, 6043, 4890, 3343, 6288, 3382, 5793, 1075, 6182, 1714, 3126, 2680, 4806, 6640, 4158, 4512, 3730, 3106, 2901, 4044, 1660, 3458, 3227, 2585, 2667, 2508, 3268, 4448, 5977, 4522, 1902, 3696, 4810, 5358, 2095, 2690, 6409, 6533, 3168, 5882, 2464, 4960, 3291, 5411, 4377, 5846, 5088, 6422, 4862, 2469, 4662, 5712, 6238, 4092, 2889, 1853, 2876, 6084, 3226, 4425, 2767, 3991, 3760, 3616, 5641, 3107, 5511, 2766, 1121, 4651, 3404, 5796, 5339, 4460, 3068, 6567, 5513, 4039, 4411, 6322, 5163, 5491, 1906, 4611, 4968, 5626, 4923, 3786, 4629, 6682, 3047, 4937, 4867, 5562, 5016, 4127, 5535, 2522, 5887, 4112, 2410, 4751, 5218, 6377, 1796, 6019, 5118, 3524, 5389, 4353, 4364, 2467, 5321, 3061, 3899, 3972, 3373, 5736, 4325, 4279, 5831, 3845, 1069, 2365, 5575, 5030, 4875, 5902, 6076, 3293, 4790, 5046, 6515, 4794, 5820, 6644, 3087, 5288, 5181, 6328, 5801, 5707, 6061, 3281, 4638, 3111, 5672, 3561, 2573, 3945, 5481, 4183, 3826, 4803, 6396, 4842, 4163, 5685, 5666, 5767, 5463, 6530, 2583, 6346, 5783, 6571, 4551, 4532, 6030, 5635, 3753, 5253, 2231, 4314, 6568, 4561, 6372, 3840, 6344, 6031, 5377, 5336, 5117, 2428, 3064, 5655, 6495, 4264, 5740, 4444, 6239, 6374, 5752, 3311, 5809, 6264, 5515, 5835, 4515, 6194, 6260, 4464, 4169, 5862, 5379, 6652, 4288, 6227, 5461, 4961, 6186, 6635, 5683, 5183, 5484, 6058, 5582]\n",
            "Cluster 12: [21, 973, 125, 3038, 392, 801, 757, 5196, 2319]\n",
            "Cluster 13: [929, 103, 279, 229, 331, 1080, 4921, 4489, 3303]\n",
            "Cluster 14: [550, 651, 419, 515, 1092, 1750, 2297, 2685, 1603, 1045, 506, 1716, 942, 685, 1523, 1095, 161, 712, 258, 717, 1646, 1458, 147, 841, 563, 839, 276, 615, 536, 637, 200, 3340, 901, 715, 1394, 213, 56, 951, 2154, 5967, 76, 104, 1031, 1521, 920, 705, 127, 5633, 3805, 233, 1320, 2081, 285, 1295, 1274, 881, 293, 2781, 5766, 2005, 840, 27, 1832, 761, 1705, 900, 1486, 259, 368, 832, 1133, 166, 3862, 487, 2357, 6253, 116, 11, 1228, 633, 745, 5930, 1236, 3351, 978, 551, 1844, 4120, 3380, 2048, 1248, 366, 1508, 559, 5728, 1269, 1925, 2613, 4085, 4818, 3274, 947, 3633, 2278, 1818, 78, 2074, 591, 436, 1307, 1126, 1438, 1395, 4361, 1296, 1129, 1627, 3073, 4145, 1648, 1507, 3093, 1599, 4635, 4119, 2762, 2936, 243, 2059, 1538, 5254, 2051, 2364, 3362, 4274, 3136, 3262, 4245, 2010, 1513, 2292, 1509, 818, 5173, 3312, 5178, 2164, 520, 3209, 2214, 3238, 3944, 4040, 4167, 582, 2984, 3888, 3516, 3584, 2294, 3806, 3205, 2954, 1125, 2817, 5062, 3419, 2291, 1637, 1931, 2139, 3768, 4134, 580, 1266, 3967, 2759, 1362, 429, 5550, 5563, 5973, 4057, 3550, 3315, 5009, 2368, 1097, 2090, 1156, 5054, 2985, 641, 1350, 1997, 4135, 4705, 3777, 3033, 5015, 6433, 2932, 2107, 4064, 1868, 2888, 3359, 4892, 2375, 2017, 1965, 2622, 3416, 2569, 2510, 183, 3066, 1707, 6536, 2268, 2803, 2795, 1337, 2130, 2853, 1578, 4115, 3667, 2843, 1838, 3084, 5386, 2867, 2854, 3151, 5695, 3794, 4801, 4730, 1289, 4533, 2829, 1797, 1657, 3927, 4227, 3464, 4241, 1391, 2058, 6430, 1687, 3135, 3438, 1889, 2221, 6281, 5063, 3825, 6506, 4938, 4191, 2855, 4518, 4405, 2758, 4229, 3767, 3235, 5743, 3092, 6580, 4344, 3615, 4000, 2052, 5533, 2333, 6548, 2530, 3939, 3239, 3798, 4589, 6686, 2918, 3488, 2887, 5290, 4778, 5662, 2271, 2765, 2578, 6280, 3542, 5812, 5521, 3855, 4613, 2664, 3581, 6649, 5140, 2570, 2774, 5911, 5546, 5568, 5039, 4272, 4901, 4463, 3943, 2298, 4268, 3530, 3196, 3429, 3680, 3932, 1595, 5132, 4139, 4179, 3202, 4840, 3757, 3504, 4022, 6091, 2280, 2798, 4781, 4131, 6269, 6369, 5674, 3204, 1770, 5107, 4476, 5079, 6638, 6113, 5954, 4643, 3535, 4755, 5312, 5664, 3067, 5430, 1800, 5684, 5464, 2807, 4493, 5917, 5848, 4400, 3654, 4312, 6542, 6412, 5456, 3929, 5337, 5331, 5749, 6054, 5941, 6190, 5214, 5690, 4462, 6080, 6242, 4663, 6510, 3693, 5775, 4190, 3817, 5246, 6125, 6552, 3956, 5085, 4744, 6132, 6669, 4577, 4180, 1021, 2264, 5495, 4837, 4504, 3634, 6022, 6313, 2740, 5948, 5040, 6246, 4631, 6228, 5697, 4168, 4934, 6284, 4018, 6068, 4899, 5082, 6285, 5302, 4337, 5817, 2623, 5202, 5792, 5961, 5599, 3901, 5959, 5258, 4866, 5352, 5617, 2158, 3796, 6358, 6052, 5138, 5468, 4262, 5837, 5860, 5394, 6321, 6118, 5650, 2595, 6545, 4257, 4420, 4133, 5378, 5224, 5644, 6116, 5701, 4263, 4636, 4746, 5154, 5201, 5261, 6326, 6005, 6443, 4775, 4278, 3273, 5126, 3313, 5567, 5768, 5522, 3954, 5345, 6123, 6440, 5489, 5525, 5480, 5966, 2328]\n",
            "Cluster 15: [1490, 1598, 230, 991, 855, 2113, 1201, 603, 2083, 2751, 3585, 4181, 5151]\n",
            "Cluster 16: [4932, 349, 1247, 589, 869, 1365, 1431, 13, 2586, 714, 622, 84, 2731, 731, 1544, 1219, 549, 2609, 682, 1731, 1457, 353, 988, 2561, 3240, 632, 1147, 1606, 3973, 295, 4102, 3041, 5839, 1682, 6305, 2384, 3979, 2645, 2408, 4877, 6605, 6641, 4445, 5813, 5668, 6188]\n",
            "Cluster 17: [3, 1658, 987, 1571, 221, 1363, 2471, 414, 1787, 3841, 1265, 4624, 5144, 5799, 1583]\n",
            "Cluster 18: [548, 112, 1141, 2538, 1001, 2169, 2332, 667, 247, 1326, 783, 1060, 1000, 1168, 129, 1085, 1424, 490, 1255, 435, 144, 462, 910, 363, 1179, 1686, 1430, 2655, 1739, 431, 298, 305, 132, 599, 1272, 1094, 3983, 776, 813, 3745, 44, 666, 1469, 1226, 4224, 1691, 5216, 893, 1330, 270, 1070, 754, 468, 2196, 879, 617, 1028, 2260, 675, 5639, 1227, 2483, 3270, 1789, 1073, 4386, 1170, 965, 2302, 2145, 2500, 450, 621, 1205, 4962, 1436, 484, 2776, 4702, 5274, 1897, 1801, 1619, 708, 257, 3503, 2605, 1002, 3046, 2400, 1862, 2366, 2710, 6631, 1003, 3237, 4439, 4240, 3605, 2125, 6021, 4467, 1963, 1275, 3406, 3552, 3165, 465, 2099, 6112, 3345, 3640, 3008, 1996, 966, 2565, 261, 3994, 1882, 6104, 3707, 5372, 4632, 1697, 1665, 1336, 3346, 1432, 1441, 3455, 6527, 3843, 2636, 1995, 3948, 3669, 2657, 3245, 3489, 4996, 2828, 2961, 4653, 1982, 2412, 825, 2359, 3996, 1251, 2794, 1038, 4661, 2327, 5806, 4626, 2211, 2421, 5832, 140, 1592, 4422, 6427, 1842, 5028, 1839, 3251, 2646, 2480, 2620, 6301, 1945, 1191, 4218, 3946, 4084, 4782, 3672, 6441, 1718, 2197, 3570, 4407, 1361, 6565, 2432, 2523, 1322, 2001, 5256, 1755, 4008, 6146, 3349, 2195, 4052, 1654, 3512, 2191, 3997, 2965, 3096, 6573, 6295, 4894, 6006, 3831, 2602, 2127, 4610, 2788, 4748, 1560, 3112, 1843, 2619, 4991, 2541, 1864, 2507, 6164, 2848, 4030, 5259, 3957, 3180, 5714, 2463, 3203, 4442, 4381, 4717, 4595, 4648, 4461, 4374, 3571, 5024, 5371, 6472, 2062, 5708, 4957, 3642, 1276, 5134, 5496, 2824, 4786, 2438, 2097, 6333, 5769, 2229, 5597, 6121, 2060, 3765, 3864, 5621, 5619, 5904, 3907, 977, 3003, 4656, 4378, 3889, 4006, 4817, 1559, 3208, 4198, 5855, 4151, 4980, 3145, 5050, 4221, 3462, 6579, 2582, 3913, 3966, 5990, 4598, 2968, 3514, 3660, 4483, 5051, 6310, 3526, 5539, 6489, 2847, 5565, 5104, 4918, 5498, 3310, 3955, 4433, 5852, 5507, 6616, 4495, 5581, 3062, 4023, 4585, 3733, 5200, 5814, 6526, 5875, 5370, 3599, 3721, 4099, 4797, 2183, 4868, 4582, 2458, 4902, 2920, 2741, 4485, 3477, 3031, 4965, 6452, 5223, 2087, 6138, 6145, 4182, 2528, 4502, 6166, 5122, 5209, 6089, 5861, 6046, 6277, 6120, 5834, 4346, 5047, 5772, 4205, 6352, 4471, 6468, 6143, 2999, 3734, 3579, 3219, 5691, 6150, 4165, 5212, 5673, 4369, 5482, 4681, 6200, 4037, 5192, 6114, 6477, 3566, 5380, 5064, 5802, 5149, 5840, 4814, 6474, 6426, 5346, 5794, 4823, 4201, 4230, 3113, 5916, 5276, 6131, 5129, 6241, 4622, 6287, 4100, 6413, 4873, 5506, 6183, 5517, 6082, 3881, 5763, 5136, 3587, 5705, 6394, 4905, 4861, 4239, 3275, 1754, 3915, 6207, 4783, 6236, 4741, 5199, 6014, 5188, 6066, 5577, 6258, 6400, 5833, 3507, 1278, 5561, 5897, 5002, 6037, 6436, 4217, 6563, 6191, 5271, 5629, 3176, 6554]\n",
            "Cluster 19: [466, 597, 476, 1644, 2898, 3249, 1531, 3890, 1241, 5087, 3746]\n",
            "Cluster 20: [272, 2140, 2265, 1656, 202, 2227, 2290, 2331, 4194, 5375, 2952, 3744, 5952, 5280, 4930]\n",
            "Cluster 21: [222, 923, 87, 1547, 950, 1284, 721, 573, 5, 3186, 262, 571, 239, 847, 782, 4987, 557, 736, 912, 1186, 265, 1139, 642, 34, 182, 175, 334, 156, 779, 1244, 890, 590, 567, 351, 1348, 1586, 1037, 1230, 903, 302, 1876, 432, 3434, 251, 1370, 725, 2587, 152, 23, 850, 150, 3030, 301, 738, 1582, 1467, 424, 4897, 803, 4858, 1298, 1263, 531, 3034, 639, 172, 4608, 5432, 4757, 1127, 1736, 4855, 3984, 1059, 1277, 2157, 1250, 560, 79, 687, 2362, 1196, 2576, 2022, 6484, 981, 3393, 593, 4618, 1260, 403, 2167, 1659, 1287, 2656, 1439, 2688, 1640, 4692, 2572, 1112, 1297, 1617, 167, 2630, 2908, 1991, 2118, 1616, 4029, 5397, 2784, 53, 2034, 1561, 1664, 5603, 3042, 3372, 4771, 3015, 1675, 713, 4149, 4428, 5850, 3710, 6219, 6279, 1494, 5918, 3328, 3779, 4511, 1815, 2011, 2494, 2305, 4974, 4770, 1926, 4931, 2436, 3847, 3500, 3261, 3905, 1567, 2553, 2735, 4284, 4048, 91, 5896, 4531, 1047, 6601, 4280, 2073, 3242, 3435, 5759, 1381, 2670, 2599, 306, 3283, 148, 2928, 1033, 5580, 1329, 2906, 1901, 4927, 2424, 2723, 1517, 3683, 2703, 1949, 1229, 3292, 2039, 2641, 3279, 3562, 3218, 2958, 5925, 3952, 1828, 4926, 3282, 2257, 4685, 3727, 3560, 2552, 3849, 1488, 3725, 1410, 1936, 4414, 2314, 4695, 3424, 3375, 905, 4474, 5825, 1908, 3976, 1952, 2677, 2700, 4395, 4889, 2722, 1855, 2335, 2143, 4097, 5706, 5933, 2248, 6608, 4140, 5058, 3851, 6442, 3258, 5089, 4822, 3717, 4175, 2804, 3065, 2270, 3589, 4736, 2161, 2635, 1584, 5908, 2077, 2923, 6458, 4517, 4715, 3910, 4410, 2589, 4480, 2489, 3705, 5373, 1999, 5447, 2973, 5309, 5094, 4924, 5573, 1625, 2239, 6303, 6508, 6039, 4352, 4952, 3749, 3162, 4348, 5360, 3916, 2340, 4415, 6221, 3366, 5042, 1738, 2896, 4090, 5257, 3914, 5220, 5661, 3622, 5013, 6359, 3787, 4016, 3978, 3286, 3697, 5101, 5609, 5699, 3265, 3378, 3482, 4596, 3968, 4247, 3192, 4293, 4083, 2117, 5141, 3628, 5937, 2669, 6432, 2640, 6062, 3800, 4309, 4820, 5109, 2082, 4580, 6371, 2445, 5624, 4949, 4756, 3485, 4059, 4742, 4619, 2413, 6499, 4219, 3773, 6134, 6453, 3304, 5905, 5210, 3470, 6564, 4260, 1427, 6664, 3103, 5804, 3289, 6607, 4001, 3877, 3784, 6678, 4216, 4455, 6071, 2316, 4271, 5278, 3536, 5135, 5725, 2219, 5890, 266, 2591, 4170, 2996, 5362, 4305, 6546, 5475, 4857, 4762, 5364, 4569, 6642, 4673, 4542, 5213, 6169, 6273, 5717, 5591, 3644, 6212, 6598, 3987, 6341, 3361, 4254, 5406, 3608, 6255, 6693, 1639, 3625, 3052, 4981, 5065, 5314, 6486, 3331, 2868, 5316, 4070, 5083, 5041, 6248, 6257, 6320, 2492, 5403, 6096, 4768, 3105, 5478, 4687, 6683, 4021, 5910, 4944, 5781, 5865, 5408, 5462, 5158, 6627, 5750, 4338, 5686, 3525, 5854, 5332, 5325, 5585, 4956, 6214, 4789, 5409, 5297, 6008, 3751, 6668, 1944, 5877, 6570, 6034, 4780, 5790, 5659, 5457, 4734, 6417, 3617]\n",
            "Cluster 22: [877, 1124]\n",
            "Cluster 23: [798, 816, 3201, 1674, 2820, 1206, 1772, 1626, 952, 5369, 5634, 6049]\n",
            "Cluster 24: [209, 535, 1310, 635, 4824]\n",
            "Cluster 25: [226, 944, 741, 189, 2895, 1799, 1699, 287, 1280, 1535, 5285]\n",
            "Cluster 26: [488, 743, 273, 1451, 1204, 1781, 4332, 284, 134, 1359, 1386, 1325, 1806, 1837, 955, 1751, 1711, 1542, 210, 1108, 3523, 4199, 5919, 5221, 2133, 1416, 2223, 1836, 3848, 3060, 4331, 5137, 4688, 5133]\n",
            "Cluster 27: [862, 248, 2823, 1661, 908, 278, 2178, 513, 2972]\n",
            "Cluster 28: [504, 395, 1745, 5198, 4350, 5913]\n",
            "Cluster 29: [543, 786, 481, 6167, 2691, 4740]\n",
            "Cluster 30: [1285, 1078, 690, 1568, 1911, 3016]\n",
            "Cluster 31: [308]\n",
            "Cluster 32: [3133, 211, 889]\n",
            "Cluster 33: [960, 6270]\n",
            "Cluster 34: [2482, 5182, 2025, 240, 769, 1017, 142, 752, 1433, 1756, 1729, 853, 123, 4193, 422, 1029, 1591, 744, 1898, 4832, 1184, 4900, 5169, 1134, 3682, 3662]\n",
            "Cluster 35: [612, 153, 117, 2, 1154, 337, 36, 1151, 5921, 217, 528, 749, 326, 1202, 4311, 1558, 2625, 1215, 1727, 2819, 3407, 2262, 6612, 80, 6007, 1593, 2979, 2441, 4452, 6218]\n",
            "Cluster 36: [1222, 412, 579, 452, 1522, 812, 382, 1034, 1209, 489, 1643, 377, 1374, 378, 3908, 2634, 672, 252, 1562, 35, 1183, 1331, 511, 821, 1472, 1169, 2417, 118, 3795, 982, 3317, 401, 4103, 30, 105, 874, 2115, 6424, 5114, 3467, 2737, 6035, 6092, 6378, 2704, 4516]\n",
            "Cluster 37: [1189, 1062, 856, 4283, 6291, 1865]\n",
            "Cluster 38: [1408, 2770, 464, 423, 931, 4689, 2454, 5734]\n",
            "Cluster 39: [62, 341, 274, 564, 1022, 66, 1193, 1216, 216, 2163, 2244, 1420, 4567, 747, 2347, 5898]\n",
            "Cluster 40: [3342, 113, 1397, 198, 1922, 1342, 1822, 220, 471, 1291, 201, 4883, 283, 3426, 3658, 3305, 5170, 5499]\n",
            "Cluster 41: [6586, 169, 438, 15, 115, 1088, 1993, 110, 694, 1142, 249, 1978, 2210, 1218, 5023, 4054, 6459]\n",
            "Cluster 42: [863, 517, 5762]\n",
            "Cluster 43: [205, 59, 60, 638, 48, 1824, 4109, 387, 427, 364, 310, 164, 882, 178, 1072, 417, 957, 1116, 533, 1090, 1767, 85, 859, 993, 1194, 805, 2643, 176, 1384, 596, 1050, 3430, 1685, 1748, 1998, 1485, 365, 1461, 1986, 653, 3643, 5031, 1713, 819, 1015, 18, 2394, 810, 1635, 647, 6611, 5113, 3508, 2592, 916, 1053, 4020, 4007, 4646, 4776, 2763, 5805, 5422]\n",
            "Cluster 44: [1, 1455, 3144]\n",
            "Cluster 45: [1321, 1239, 1118, 139, 1042, 1399, 2212, 237, 858, 3321, 4964, 3498, 3586, 4592, 2356, 5036, 6213]\n",
            "Cluster 46: [101, 86, 552, 771, 1135, 300, 1056, 94, 1946, 932, 1067, 3799, 3596, 4065, 3506, 1396, 1470]\n",
            "Cluster 47: [5612, 360, 1323, 81, 475, 45, 1131, 1020, 332, 2431, 1132, 1732, 1540, 453, 1076, 4529, 516, 1232, 1187, 1212, 2402, 3441, 5424, 817, 137, 1887, 3355, 4256, 4660, 2076, 6415, 3999, 5777, 5172]\n",
            "Cluster 48: [1081, 1565, 907, 2067, 2269, 6585]\n",
            "Cluster 49: [6204, 565, 728, 1150, 309, 1223, 1380, 664, 2955, 1881, 2913, 1930, 1746, 5534, 5293]\n"
          ]
        }
      ],
      "source": [
        "def K_Means_Clustering(user_data):\n",
        "    user_vectors = {}\n",
        "\n",
        "    for region in range(1, 78):\n",
        "        checkins_region = checkins[checkins['neighbourhood_id'] == region]\n",
        "        \n",
        "        if checkins_region.shape[0] > 0:\n",
        "            user_ids = checkins_region['userid'].unique()\n",
        "            for user_id in user_ids:\n",
        "                user_vectors[user_id] = [0] * 9  # Initialize vector of size 9 with zeros\n",
        "\n",
        "            crimes_region = user_data[user_data['neighbourhood_id'] == region]\n",
        "            for user_id in user_ids:\n",
        "                user_crimes = crimes_region[crimes_region['uid'] == user_id]\n",
        "                for month in range(4, 13):  # From April (4) to December (12)\n",
        "                    start_date = pd.Timestamp(year=2019, month=month, day=1)\n",
        "                    if month == 12:\n",
        "                        end_date = pd.Timestamp(year=2019, month=month, day=31, hour=23, minute=59, second=59)\n",
        "                    else:\n",
        "                        end_date = pd.Timestamp(year=2019, month=month+1, day=1) - pd.Timedelta(seconds=1)\n",
        "                    \n",
        "                    crime_count = user_crimes[(user_crimes['time'] >= start_date) & (user_crimes['time'] <= end_date)].shape[0]\n",
        "                    user_vectors[user_id][month-4] = crime_count  # Fill the vector for the corresponding month\n",
        "    \n",
        "\n",
        "    # Convert user_vectors dictionary to a list of vectors\n",
        "    user_ids = list(user_vectors.keys())\n",
        "    vectors = list(user_vectors.values())\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=cluster_head_count, random_state=0).fit(vectors)\n",
        "\n",
        "    # Create a dictionary to store the clusters\n",
        "    clusters = {i: [] for i in range(cluster_head_count)}\n",
        "\n",
        "    # Assign user_ids to their respective clusters\n",
        "    for idx, label in enumerate(kmeans.labels_):\n",
        "        clusters[label].append(user_ids[idx])\n",
        "\n",
        "    # Print the clusters\n",
        "    for cluster_id, users in clusters.items():\n",
        "        print(f\"Cluster {cluster_id}: {users}\")\n",
        "\n",
        "\n",
        "K_Means_Clustering(user_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls0qXzLzPGkA"
      },
      "source": [
        "### ~~Now from the train data, seperate the server portion and user portion ( perviously done) ~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jODAp_nvPGkA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Step 2: Split train data into server and user based on beta percentage for each neighbourhood_id\n",
        "# # server_percentage = 0.25\n",
        "# # server_percentage = 0.70 # percent of data in server\n",
        "\n",
        "# # Set the seed\n",
        "# seed_value = 46\n",
        "# np.random.seed(seed_value)\n",
        "\n",
        "# #rename the Community Area to neighbourhood_id\n",
        "# train_data.rename(columns={\"Community Area\": \"neighbourhood_id\"}, inplace=True)\n",
        "# # Group by neighbourhood_id\n",
        "# grouped_train_data = train_data.groupby('neighbourhood_id')\n",
        "\n",
        "# server_data = pd.DataFrame()  # To store the server data\n",
        "# user_data = pd.DataFrame()    # To store the user data\n",
        "\n",
        "# # Iterate through each group\n",
        "# for group_name, group_data in grouped_train_data:\n",
        "#     # Shuffle the rows within each group\n",
        "#     shuffled_group_data = group_data.sample(frac=1, random_state=42)  # Set a random_state for reproducibility\n",
        "\n",
        "#     # Split based on beta percentage\n",
        "#     server_rows = shuffled_group_data.head(int(server_percentage * len(shuffled_group_data)))\n",
        "#     user_rows = shuffled_group_data.tail(len(shuffled_group_data) - len(server_rows))\n",
        "\n",
        "#     # Concatenate the server and user data for each group\n",
        "#     server_data = pd.concat([server_data, server_rows])\n",
        "#     user_data = pd.concat([user_data, user_rows])\n",
        "\n",
        "# # Now, 'server_data' and 'user_data' contain the final server and user data respectively\n",
        "# # here the server and user data are both \"train\" data which are non overlapping\n",
        "# #TODO: we will assign uid to the user_data now\n",
        "# CHECKIN_PATH = './checkins_with_neighborhood_id.csv'\n",
        "# checkins = pd.read_csv(CHECKIN_PATH)\n",
        "# checkins.drop(['venueid', 'venue_category', 'count'], axis=1, inplace=True)\n",
        "# checkins['utctime'] = pd.to_datetime(checkins['utctime']).dt.tz_localize(None)\n",
        "# checkins.rename(columns={\"utctime\": \"time\"}, inplace=True)\n",
        "# checkins.sort_values(by=['time'], inplace=True)\n",
        "\n",
        "# # client_list_500 = list(range(1, 501))  # Creates a list from 1 to 500 (inclusive)\n",
        "# # np.random.shuffle(client_list_500)\n",
        "# def assign_uid_to_crime():\n",
        "#     for i in range(1,78):\n",
        "#         crimes_i = user_data[user_data['neighbourhood_id'] == i]\n",
        "#         checkins_i = checkins[checkins['neighbourhood_id'] == i]\n",
        "#         print(f\"region:  {i}, crimes :{crimes_i.shape[0]}, checkins: {checkins_i.shape[0]}, uid: {checkins_i['userid'].unique().shape[0]}\")\n",
        "#         if(checkins_i['userid'].unique().shape[0] > 0):\n",
        "#             # Shuffle the array to ensure randomness\n",
        "#             np.random.shuffle(checkins_i['userid'].unique())\n",
        "\n",
        "#             # Take the first cluster_head_count values\n",
        "#             unique_user_idssss = checkins_i['userid'].unique()[:cluster_head_count]\n",
        "#             # print('unique user iddss ',len(unique_user_idssss))\n",
        "#             actual_unique_user_length = len(unique_user_idssss)\n",
        "#             # print(f'len of actual uniq users for ${i} ',actual_unique_user_length)\n",
        "#             # selected_uids = set()\n",
        "#             index_count = 0\n",
        "#             for index, row in crimes_i.iterrows():\n",
        "#                 # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid']))\n",
        "#                 # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid'].unique()[:cluster_head_count]))\n",
        "#                 # user_data.loc[index, 'uid'] = int(np.random.choice(20))\n",
        "#                 # Ensure that we loop back to the beginning of the array if we reach its end\n",
        "#                 uid = int(unique_user_idssss[index_count % actual_unique_user_length])\n",
        "#                 # selected_uids.add(uid)\n",
        "#                 user_data.loc[index, 'uid'] = uid\n",
        "#                 index_count+=1\n",
        "\n",
        "#             # print(f'selected uid for region {i} is ',len(selected_uids))\n",
        "#         else :\n",
        "#             for index, row in crimes_i.iterrows():\n",
        "#                 user_data.loc[index, 'uid'] = -1\n",
        "#         # unique_user_idssss = client_list_500[:cluster_head_count]\n",
        "#         # unique_user_idssss = checkins_i['userid'].unique()[:cluster_head_count]\n",
        "#         # actual_unique_user_length = len(unique_user_idssss)\n",
        "#         # for index, row in crimes_i.iterrows():\n",
        "#         #     uid = int(unique_user_idssss[index % actual_unique_user_length])\n",
        "#         #     user_data.loc[index, 'uid'] = uid\n",
        "#     print('uid assignment for all the 77 regions done,but there are -1 uid also so we have to replace them with those uids which have very low crime incidents')\n",
        "#     uid_row_count = user_data['uid'].value_counts().reset_index()\n",
        "#     uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "#     # Sort the DataFrame by 'uid'\n",
        "#     uid_row_count = uid_row_count.sort_values(by='crime_count')\n",
        "\n",
        "\n",
        "#     # Get the top 1946 uid values\n",
        "#     top_uid_list = uid_row_count.head(1946)['uid'].tolist()\n",
        "#     # print('top uid list ',top_uid_list)\n",
        "\n",
        "#     # Read the DataFrame where uid == -1\n",
        "#     # df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "\n",
        "#     # # Assign a randomly chosen uid from the top 1946 uid list\n",
        "#     # df_uid_minus_one['uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "#     # df_uid_minus_one['uid'] = df_uid_minus_one['uid'].astype(int)\n",
        "\n",
        "#     # Use .loc to update the DataFrame where uid == -1\n",
        "#     df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "#     user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "#     # user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=False)\n",
        "#     user_data['uid'] = user_data['uid'].astype(int)\n",
        "\n",
        "#     print('--------------- done with replacing -1 with the random uid which has very less data (random from bottom, around 1946 unique uids which has very very less crime data)')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B9553Dn1PGkB",
        "outputId": "0bc2ab25-fdbe-4449-cbc0-9fb7e9034014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------- done with putting uids of cluster heads to all the crime incidents of all the regions\n",
            "                    time  IUCR        Primary Type  neighbourhood_id  \\\n",
            "1678 2019-08-03 01:00:00  1320     CRIMINAL DAMAGE                 1   \n",
            "2325 2019-04-07 17:00:00  1130  DECEPTIVE PRACTICE                 1   \n",
            "1757 2019-11-20 19:02:00  0320             ROBBERY                 1   \n",
            "\n",
            "            lat        lon  TYPE_ID  uid  \n",
            "1678  42.012523 -87.680949        2  290  \n",
            "2325  42.006997 -87.675288        0   75  \n",
            "1757  42.001065 -87.661254        4  101  \n",
            "-------------- uid assignment done --------------------\n",
            "----------- user data and server data saved in csv -------------\n",
            "-------------user crime count also stored -------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set the seed\n",
        "seed_value = 46\n",
        "np.random.seed(seed_value)\n",
        "# assign_uid_to_crime()\n",
        "print(user_data.head(3))\n",
        "print('-------------- uid assignment done --------------------')\n",
        "\n",
        "# ------------------ uid assign done -------------------------\n",
        "# Step 3: Isolate a certain portion of uid (isolated_users_percentage) with no data shared with the server\n",
        "# user_data['uid'] = user_data['uid'].astype(int)\n",
        "unique_uids = user_data['uid'].unique()\n",
        "isolated_uids = np.random.choice(unique_uids, size=int(isolated_users_percentage * len(unique_uids)), replace=False)\n",
        "isolated_uids = isolated_uids.astype(int)\n",
        "# isolated_user_data = user_data[user_data['uid'].isin(isolated_uids)]\n",
        "\n",
        "# Step 4: Generate new overlapping user data\n",
        "\n",
        "#make sure that the neighbourhood_id is integer\n",
        "server_data['neighbourhood_id'] = server_data['neighbourhood_id'].astype(int)\n",
        "\n",
        "new_overlapping_user_data = pd.DataFrame()\n",
        "\n",
        "if(overlapping_percentage != 0):\n",
        "    # Iterate through each group for server data\n",
        "    for group_name, group_data in server_data.groupby('neighbourhood_id'):\n",
        "        # Extract overlapping_percentage percentage of random server data\n",
        "        extracted_group_data = group_data.sample(frac=overlapping_percentage, random_state=42)\n",
        "\n",
        "        # Get the unique uid for the current group\n",
        "        unique_uids_for_group = user_data[user_data['neighbourhood_id'] == group_name]['uid'].unique()\n",
        "\n",
        "        # Assign a random uid from the remaining uids to each data point in the extracted group data\n",
        "        remaining_uids = np.setdiff1d(unique_uids_for_group, isolated_uids)\n",
        "        if remaining_uids.size != 0:\n",
        "            extracted_group_data['uid'] = np.random.choice(remaining_uids, size=len(extracted_group_data), replace=True)\n",
        "            extracted_group_data['uid'] = extracted_group_data['uid'].astype(int)\n",
        "\n",
        "        # Concatenate with the new overlapping user data\n",
        "        new_overlapping_user_data = pd.concat([new_overlapping_user_data, extracted_group_data])\n",
        "\n",
        "    # Concatenate the new overlapping user data with the isolated user data to get the final user data\n",
        "    user_data = pd.concat([user_data, new_overlapping_user_data])\n",
        "\n",
        "# Now, 'server_data' and 'user_data' contain the final server and user data respectively\n",
        "server_data.to_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv', index=False)\n",
        "user_data.to_csv(f'./crimes_with_uid_{save_file_path}.csv', index=False)\n",
        "print('----------- user data and server data saved in csv -------------')\n",
        "\n",
        "uid_row_count = user_data['uid'].value_counts().reset_index()\n",
        "uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "uid_row_count = uid_row_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "uid_row_count.to_csv(f'user_crimes_count_{save_file_path}.csv', index=False)\n",
        "print('-------------user crime count also stored -------------------------')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t3fH-CeiPGkC",
        "outputId": "a9a53dfc-d651-4418-b5bc-f5e22f5773c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------region crime count also stored -------------------------\n"
          ]
        }
      ],
      "source": [
        "# getting the region wise crime list to sort which region has greater crime incidents\n",
        "user_data_for_new_calc = pd.read_csv(f'./crimes_with_uid_{save_file_path}.csv')\n",
        "region_crime_count = user_data_for_new_calc['neighbourhood_id'].value_counts().reset_index()\n",
        "region_crime_count.columns = ['region_id', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "region_crime_count = region_crime_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "region_crime_count.to_csv(f'region_crime_counts_{save_file_path}.csv', index=False)\n",
        "print('-------------region crime count also stored -------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ZGFoELAPGkD",
        "outputId": "ca222d27-95e1-4995-d3a8-da171518515c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "corresponding_uids_data = pd.read_csv(root_folder+f'/user_crimes_count_{save_file_path}.csv')\n",
        "corresponding_uids = corresponding_uids_data['uid'].to_numpy()\n",
        "print(len(corresponding_uids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsSiDXpdPGkD"
      },
      "source": [
        "#### User Train Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Kwk7kj71PGkE",
        "outputId": "203386ba-8874-47e5-8647-df335425dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user id completed :  130\n",
            "user id completed :  64\n",
            "user id completed :  319\n",
            "user id completed :  886\n",
            "user id completed :  79\n",
            "user id completed :  71\n",
            "user id completed :  411\n",
            "user id completed :  695\n",
            "user id completed :  1210\n",
            "user id completed :  75\n",
            "user id completed :  0\n",
            "user id completed :  538\n",
            "user id completed :  12\n",
            "user id completed :  16\n",
            "user id completed :  127\n",
            "user id completed :  116\n",
            "user id completed :  1083\n",
            "user id completed :  180\n",
            "user id completed :  290\n",
            "user id completed :  6433\n",
            "user id completed :  198\n",
            "user id completed :  394\n",
            "user id completed :  101\n",
            "user id completed :  77\n",
            "user id completed :  456\n",
            "user id completed :  276\n",
            "user id completed :  232\n",
            "user id completed :  165\n",
            "user id completed :  3487\n",
            "user id completed :  2022\n",
            "user id completed :  11\n",
            "user id completed :  63\n",
            "user id completed :  122\n",
            "user id completed :  3132\n",
            "user id completed :  997\n",
            "user id completed :  5043\n",
            "user id completed :  633\n",
            "user id completed :  477\n",
            "user id completed :  323\n",
            "user id completed :  1114\n",
            "user id completed :  89\n",
            "user id completed :  120\n",
            "user id completed :  683\n",
            "user id completed :  88\n",
            "user id completed :  73\n",
            "user id completed :  184\n",
            "user id completed :  618\n",
            "user id completed :  572\n",
            "user id completed :  211\n",
            "user id completed :  2047\n"
          ]
        }
      ],
      "source": [
        "# # now we will create the side dataset based on data/chicago/side_com_adj.txt file\n",
        "'''from here we will need the rest of the codes,'''\n",
        "# from tqdm.notebook import tqdm, trange\n",
        "import shutil\n",
        "\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('09/19/2019 07:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "# read the data lat lon file\n",
        "# user_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "user_crime_full_df = pd.read_csv(f'./crimes_with_uid_{save_file_path}.csv')\n",
        "user_crime_full_df['time'] = pd.to_datetime(user_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# find the unique user ids from this df\n",
        "# unique_user_ids = user_crime_full_df['uid'].unique()\n",
        "#from all the unique user ids, take 1000 user_ids that are not present in the corresponding uids\n",
        "# Filter out 1000 uids that are not present in corr_uids\n",
        "# filtered_uids_for_server_data = [uid for uid in unique_user_ids if uid not in corresponding_uids][:2000]\n",
        "# get the rest uids for cross_mixing\n",
        "# cross_mixing_uids = [uid for uid in unique_user_ids if uid not in filtered_uids_for_server_data and uid not in corresponding_uids]\n",
        "\n",
        "\n",
        "#now get the dataframe where the uids are the cross_mixing_uids\n",
        "# cross_mixing_df = user_crime_full_df[user_crime_full_df['uid'].isin(cross_mixing_uids)]\n",
        "\n",
        "# print('unique user ids are ',len(unique_user_ids))\n",
        "# sort this unique_user_ids in ascending order\n",
        "# unique_user_ids.sort()\n",
        "# taken_user_ids = 1015  # here many users are absent so we took 1015 to get approximately 1000 users\n",
        "# unique_user_ids = unique_user_ids[:taken_user_ids]\n",
        "# for each user id, create a directory in \"../data/chicago2/\" directory named that particular user id if the directory doesn't exist already\n",
        "# for id in trange(len(corresponding_uids)):\n",
        "for id in range(len(corresponding_uids)):\n",
        "    user_id = corresponding_uids[id]\n",
        "    # create the directory\n",
        "    # os.makedirs('/data/chicago/user/' + str(user_id), exist_ok=True)\n",
        "    # create a new df for each user id\n",
        "    user_crime_df = user_crime_full_df[user_crime_full_df['uid'] == user_id]\n",
        "\n",
        "    total_meighbors = 77\n",
        "    # total_days = 260\n",
        "    # total_days = 365  # calculted from START_DATE and END_DATE,not 260\n",
        "    # total_days = 261  # calculted from START_DATE and END_DATE,not 365, (END_DATE - START_DATE)  (excludes the END_DATE)\n",
        "    total_days = 275  # calculted from START_DATE and END_DATE,not 261, (END_DATE - START_DATE)  (includes the END_DATE)\n",
        "    time_division_per_day = 6 # here each day is divided in 12 hrs slot, so each day has 2hrs slot\n",
        "    hours_per_cell  = 4\n",
        "    total_cols = total_days * time_division_per_day\n",
        "    # now we have only first 8 hours (07:59:59) for the END_DATE, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "    # total_cols +=2\n",
        "    # total number of row is total crimes in crime_mapping\n",
        "    total_rows = len(crime_mapping)\n",
        "    # create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "    user_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "    '''code for adding the side crimes'''\n",
        "    # create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "    user_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "    # Convert the time format to datetime\n",
        "    # user_crime_df['time'] = pd.to_datetime(user_crime_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # run loop for each row\n",
        "    for index, row in user_crime_df.iterrows():\n",
        "        # get the neighbourhood id\n",
        "        neighbourhood_id = row['neighbourhood_id']\n",
        "        neighbourhood_id -=1 # for 0 based indexing\n",
        "        # get the crime type id\n",
        "        crime_type_id = row['TYPE_ID']\n",
        "        # get the date\n",
        "        date = row['time']\n",
        "        # get the time\n",
        "        time = date.time()\n",
        "        # get the day\n",
        "        day = date.date()\n",
        "        # get the time division\n",
        "        time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "        # get the day index\n",
        "        day_index = (day - START_DATE.date()).days\n",
        "        # get the time division index\n",
        "        time_division_index = day_index * time_division_per_day + time_division\n",
        "        # increment the value in the tensor\n",
        "        user_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1   # for com_crime calculations\n",
        "        cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "        user_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1   # for side crime calculations\n",
        "\n",
        "\n",
        "\n",
        "    #mixing with user data started\n",
        "    # random_sampling_percentage = 20\n",
        "    #now take random_sampling_percentage of dataframe from crime_mixing_df\n",
        "    # crime_mixing_df_for_current_user = cross_mixing_df.sample(frac=random_sampling_percentage/100)\n",
        "\n",
        "    # crime_mixing_df_for_current_user['time'] = pd.to_datetime(crime_mixing_df_for_current_user['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # run loop for each row\n",
        "    # for index, row in crime_mixing_df_for_current_user.iterrows():\n",
        "    #     # get the neighbourhood id\n",
        "    #     neighbourhood_id = row['neighbourhood_id']\n",
        "    #     neighbourhood_id -=1 # for 0 based indexing\n",
        "    #     # get the crime type id\n",
        "    #     crime_type_id = row['TYPE_ID']\n",
        "    #     # get the date\n",
        "    #     date = row['time']\n",
        "    #     # get the time\n",
        "    #     time = date.time()\n",
        "    #     # get the day\n",
        "    #     day = date.date()\n",
        "    #     # get the time division\n",
        "    #     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    #     # get the day index\n",
        "    #     day_index = (day - START_DATE.date()).days\n",
        "    #     # get the time division index\n",
        "    #     time_division_index = day_index * time_division_per_day + time_division\n",
        "    #     # increment the value in the tensor\n",
        "    #     user_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "    #     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    #     user_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "    output_directory = f'./toy_data/chicago/user_{save_file_path}/'+str(user_id)\n",
        "    if os.path.exists(output_directory):\n",
        "      shutil.rmtree(output_directory)\n",
        "\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "    for i in range(total_meighbors):\n",
        "        filename = f\"{output_directory}/r_{i}.txt\"\n",
        "        # Extract the 2D slice at index i\n",
        "        slice_2d = user_crime_tensor[i, :, :]\n",
        "\n",
        "        # Save the 2D slice as a text file\n",
        "        np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "    # now same for side data\n",
        "    for i in range(9):\n",
        "        filename = f\"{output_directory}/s_{i}.txt\"\n",
        "        # Extract the 2D slice at index i\n",
        "        slice_2d = user_side_crime_tensor[i, :, :]\n",
        "\n",
        "        # Save the 2D slice as a text file\n",
        "        np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "    print('user id completed : ',user_id)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXfF0A8mPGkF"
      },
      "source": [
        "### Server Train Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IzMAIhImPGkF",
        "outputId": "13207114-777a-4222-8840-5cf7b7a10ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here it ended\n"
          ]
        }
      ],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# dataset1 = pd.read_csv('training_crime_data_for_last_9_months.csv')\n",
        "# test_start_date = dataset1['time'].iloc[0]\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "server_crime_full_df = pd.read_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv')\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 77\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 261 # counted from START_DATE and END_DATE,(END_DATE - START_DATE) (excluding the END_DATE) not 365\n",
        "total_days = 275 # counted from START_DATE and END_DATE,(END_DATE - START_DATE) (including the END_DATE) not 261\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# now we have only first 8 hours (07:59:59) for the END_DATE, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "# total_cols +=2\n",
        "# total number of row is total crimes in crime_mapping\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "server_crime_full_df['time'] = pd.to_datetime(server_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in final_server_dataset.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = f'./toy_data/chicago/server_{save_file_path}'\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "\n",
        "os.makedirs(output_directory)\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here it ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk8Tl2UUPGkG"
      },
      "source": [
        "### Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DwMZoz6PGkG",
        "outputId": "23675464-7585-4efd-b9db-84d89cb08998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here test data generation ended\n"
          ]
        }
      ],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('09/19/2019 08:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('03/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "test_crime_full_df = pd.read_csv('./testing_crime_data_for_first_3_months.csv')\n",
        "# test_crime_full_df = test_crime_full_df.dropna()\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 77\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 103 # counted from START_DATE and END_DATE,(END_DATE - START_DATE),\n",
        "total_days = 90 # counted from START_DATE and END_DATE,(END_DATE - START_DATE),\n",
        "#! (excluding the START_DATE, because the START_DATE has not the full 24 hrs, it has hours from 08:00:00) not 365\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# total number of row is total crimes in crime_mapping\n",
        "# now we have only last 16 hours (15:59:59 precisely) for the START_DATE because the time starts from 08:00:00, so we cannot take the whole day, rather we will take additional(16/hours_per_cell) = (16/4)= 4 columns extra\n",
        "# total_cols += 4\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "test_crime_full_df['time'] = pd.to_datetime(test_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in test_crime_full_df.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    #! here we are subtracting 8 hours from the day because the START_DATE begins with time 08:00:00, we want to map 08:00:00 to 00:00:00, so we are subtracting 8 hours from the time slot for each row\n",
        "    # date = row['time'] - timedelta(hours=8)\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = \"./toy_data/chicago/test\"\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here test data generation ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86A4oDXfPGkH"
      },
      "source": [
        "### Aggregated Data Generation For Central AIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqnVw4tdPGkH",
        "outputId": "c4499f7a-19da-4cb3-9157-21d86601b606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here aggregated data generation ended\n"
          ]
        }
      ],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('09/19/2019 07:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "# full_crime_full_df = pd.read_csv('./training_crime_data_for_last_9_months.csv')\n",
        "full_crime_full_df = pd.read_csv('./Final_Training_Data.csv')\n",
        "# full_crime_full_df = full_crime_full_df.dropna()\n",
        "full_crime_full_df.rename(columns={\"Community Area\": \"neighbourhood_id\"}, inplace=True)\n",
        "\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 77\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 261 # counted from START_DATE and END_DATE,(END_DATE - START_DATE), excluding the END_DATE because we dont have whole 24 hours for the END_DATE not 365\n",
        "total_days = 275 # counted from START_DATE and END_DATE,(END_DATE - START_DATE), including the END_DATE because we dont have whole 24 hours for the END_DATE not 365\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# now we have only first 8 hours (07:59:59) for the last date, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "# total_cols +=2\n",
        "# total number of row is total crimes in crime_mapping\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "full_crime_full_df['time'] = pd.to_datetime(full_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in full_crime_full_df.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = \"./toy_data/chicago/aggregated\"\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here aggregated data generation ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKVGC2Rh3PT"
      },
      "source": [
        "# Unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_hDuE4HosXu"
      },
      "outputs": [],
      "source": [
        "# taxi_trip_df = pd.read_csv('/data/chicago/out_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# # print the shape\n",
        "# print(taxi_trip_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2VPISbhosXu"
      },
      "outputs": [],
      "source": [
        "# # here taxi data set creation code\n",
        "# # this calc is for \"taxi in data\"\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# new_start_date = datetime.strptime('2013-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# new_end_date =  datetime.strptime('2013-09-16 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# # read the Taxi trip csv\n",
        "# # ----------------------------------------\n",
        "# taxi_trip_df = pd.read_csv('/data/chicago/out_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# #-------------------------------------------\n",
        "# # filter the df to have data from new start date to end date\n",
        "# taxi_trip_df = taxi_trip_df[(taxi_trip_df['Trip Start Timestamp'] >= new_start_date) & (taxi_trip_df['Trip Start Timestamp'] <= new_end_date)]\n",
        "# # rename the start time stamp column to start time and end timestamp column to end time, Pickup Community Area to out , Dropoff Community Area to in\n",
        "# taxi_trip_df.rename(columns={'Trip Start Timestamp':'out_time', 'Pickup Community Area':'out'}, inplace=True)\n",
        "# # get the number of days\n",
        "# num_days = (new_end_date - new_start_date).days + 1 # 260\n",
        "# # get the number of time divisions\n",
        "# num_time_divisions = 2  # here time divison is 2 means each day has 2 time slots\n",
        "# # get the number of regions\n",
        "# num_regions = 77\n",
        "# # get the number of rows\n",
        "# num_rows = 2\n",
        "# # get the number of columns\n",
        "# num_cols = num_days * num_time_divisions # 260 * 2\n",
        "# # create a 3d tensor of dimension num_regions X num_rows X num_cols\n",
        "# taxi_trip_tensor = np.zeros((num_regions, num_rows, num_cols))\n",
        "# # Convert the time format to datetime\n",
        "# taxi_trip_df['out_time'] = pd.to_datetime(taxi_trip_df['out_time'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "# # run loop for each row\n",
        "# for index, row in taxi_trip_df.iterrows():\n",
        "#     # check if out and in times are within range\n",
        "#     if row['out_time'] >= new_start_date and row['out_time'] <= new_end_date:\n",
        "#         # get the out region\n",
        "#         out_region = row['out']\n",
        "#         # get the out time\n",
        "#         out_time = row['out_time']\n",
        "#         # get the out day\n",
        "#         out_day = out_time.date()\n",
        "#         # get the out time division\n",
        "#         out_time_division = int(math.floor(out_time.hour / 12))  # here each day is divided in 12 hrs time slot,\n",
        "\n",
        "#         # get the out day index\n",
        "#         out_day_index = (out_day - new_start_date.date()).days\n",
        "#         # get the out time division index\n",
        "#         out_time_division_index = out_day_index * num_time_divisions + out_time_division\n",
        "#         # get the in time division index\n",
        "#         # increment the value in the tensor\n",
        "#         taxi_trip_tensor[out_region - 1][1][out_time_division_index] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuN1KeXlosXv"
      },
      "outputs": [],
      "source": [
        "# # here taxi data set creation code\n",
        "# # this calc is for \"taxi in data\"\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# new_start_date = datetime.strptime('2013-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# new_end_date =  datetime.strptime('2013-09-16 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# # read the Taxi trip csv\n",
        "# # ----------------------------------------\n",
        "# taxi_trip_df = pd.read_csv('/data/chicago/in_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# #-------------------------------------------\n",
        "# # filter the df to have data from new start date to end date\n",
        "# taxi_trip_df = taxi_trip_df[(taxi_trip_df['Trip End Timestamp'] >= new_start_date) & (taxi_trip_df['Trip End Timestamp'] <= new_end_date)]\n",
        "# # rename the start time stamp column to start time and end timestamp column to end time, Pickup Community Area to out , Dropoff Community Area to in\n",
        "# taxi_trip_df.rename(columns={'Trip End Timestamp':'in_time', 'Dropoff Community Area':'in'}, inplace=True)\n",
        "\n",
        "# # Convert the time format to datetime\n",
        "# taxi_trip_df['in_time'] = pd.to_datetime(taxi_trip_df['in_time'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "# # run loop for each row\n",
        "# for index, row in taxi_trip_df.iterrows():\n",
        "#     # check if in and in times are within range\n",
        "#     if row['in_time'] >= new_start_date and row['in_time'] <= new_end_date:\n",
        "#         # get the in region\n",
        "#         in_region = row['in']\n",
        "#         # get the in time\n",
        "#         in_time = row['in_time']\n",
        "#         # get the in day\n",
        "#         in_day = in_time.date()\n",
        "#         # get the in time division\n",
        "#         in_time_division = int(math.floor(in_time.hour / 12)) # here dividing the day in 12 hrs, so each day has two slots indexed 0,1\n",
        "\n",
        "#         # get the in day index\n",
        "#         in_day_index = (in_day - new_start_date.date()).days\n",
        "#         # get the in time division index\n",
        "#         in_time_division_index = in_day_index * num_time_divisions + in_time_division\n",
        "#         # get the in time division index\n",
        "#         # increment the value in the tensor\n",
        "#         taxi_trip_tensor[in_region - 1][0][in_time_division_index] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltjsRwU9osXv"
      },
      "outputs": [],
      "source": [
        "# # for i in range(77):\n",
        "# #     file = open('content/drive/MyDrive/Undergrad_Thesis_Group_2_FedCrime/AIST/data/chicago2/act_ext' + '/taxi_' + str(i+1) + '.txt', 'w')\n",
        "# #     for j in range(total_rows):\n",
        "# #         for k in range(total_cols):\n",
        "# #             file.write(str(int(server_side_crime_tensor[i][j][k])) + \" \")\n",
        "# #         file.write(\"\\n\")\n",
        "# #     file.close()\n",
        "\n",
        "# output_directory = \"/data/chicago/act_ext\"\n",
        "# for i in range(77):\n",
        "#     filename = f\"{output_directory}/taxi_{i+1}.txt\"\n",
        "#     # Extract the 2D slice at index i\n",
        "#     slice_2d = taxi_trip_tensor[i, :, :]\n",
        "\n",
        "#     # Save the 2D slice as a text file\n",
        "#     np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
