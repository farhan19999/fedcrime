{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQm3G-C736GH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "root_folder = './'\n",
        "#set hidden state of the lstm\n",
        "hidden_state_of_lstm = 40\n",
        "# learning_rate = 0.0002\n",
        "learning_rate = 0.001\n",
        "server_percentage = 0.5 # percent of data in server\n",
        "# server_percentage_2 = 1.0 # percent of data in server\n",
        "# user_percentage_2 = 1.0\n",
        "isolated_users_percentage = 0.1  # Change this according to your requirement\n",
        "# isolated_users_percentage = 0.0  # Change this according to your requirement\n",
        "overlapping_percentage = 0  # Change this according to your requirement\n",
        "cluster_head_count = 25 #10,20,30,40,50\n",
        "\n",
        "save_file_path = \"k_means_clustered_\" + \"hs_\" + str(hidden_state_of_lstm) + \"_lr_\" + str(int(learning_rate * 10000)) + \"_Sp_\" + str(int(server_percentage * 100))+\"_Iu_\" + str(int(isolated_users_percentage * 100)) + \"_Op_\" + str(int(overlapping_percentage * 100)) + \"_Cl_\" + str(cluster_head_count)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_tMwCO8JHsYQ"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V1lFYxA0PGkA"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file into a DataFrame\n",
        "# train_data = pd.read_csv('./training_crime_data_for_last_9_months.csv')\n",
        "train_data = pd.read_csv('./Final_Training_Data_side.csv')\n",
        "\n",
        "# Assuming 'timestamp' is the column name for timestamp and 'neighbourhood_id' is the column for neighbourhood_id\n",
        "train_data['time'] = pd.to_datetime(train_data['time'], format='%Y-%m-%d %H:%M:%S')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now from the train data, give 100 percent data to both server and all the users, then get a certain cluster heads and assign all the crimes to those cluster heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New User Assigment to Crimes Code for New Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "region:  1, crimes :6638, checkins: 8554, uid: 995\n",
            "region:  2, crimes :3493, checkins: 3512, uid: 405\n",
            "region:  3, crimes :5658, checkins: 25509, uid: 2139\n",
            "region:  4, crimes :18675, checkins: 16497, uid: 1980\n",
            "region:  5, crimes :7730, checkins: 36888, uid: 3541\n",
            "region:  6, crimes :9667, checkins: 5304, uid: 877\n",
            "region:  7, crimes :9318, checkins: 4474, uid: 1143\n",
            "region:  8, crimes :4516, checkins: 1070, uid: 206\n",
            "region:  9, crimes :8208, checkins: 2020, uid: 307\n",
            "uid assignment for all the 77 regions done,but there are -1 uid also so we have to replace them with those uids which have very low crime incidents\n",
            "--------------- done with replacing -1 with the random uid which has very less data (random from bottom, around 1946 unique uids which has very very less crime data)\n",
            "######### ------------- All Crimes have been assigned with a User ------------- ##################\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Split train data into server and user based on beta percentage for each neighbourhood_id\n",
        "# server_percentage = 0.25 \n",
        "# server_percentage = 0.70 # percent of data in server\n",
        "\n",
        "# Set the seed\n",
        "seed_value = 46\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "#rename the Community Area to neighbourhood_id\n",
        "train_data.rename(columns={\"Community Area\": \"neighbourhood_id\"}, inplace=True)\n",
        "# Group by neighbourhood_id\n",
        "grouped_train_data = train_data.groupby('neighbourhood_id')\n",
        "\n",
        "\n",
        "server_data = pd.DataFrame()  # To store the server data\n",
        "user_data = pd.DataFrame()    # To store the user data\n",
        "\n",
        "# Iterate through each group\n",
        "for group_name, group_data in grouped_train_data:\n",
        "    # Shuffle the rows within each group\n",
        "    shuffled_group_data = group_data.sample(frac=1, random_state=42)  # Set a random_state for reproducibility\n",
        "    \n",
        "    # Split based on beta percentage\n",
        "    server_rows = shuffled_group_data.head(int(server_percentage * len(shuffled_group_data)))\n",
        "    user_rows = shuffled_group_data.tail(len(shuffled_group_data) - len(server_rows))\n",
        "\n",
        "    # Concatenate the server and user data for each group\n",
        "    server_data = pd.concat([server_data, server_rows])\n",
        "    user_data = pd.concat([user_data, user_rows])\n",
        "\n",
        "# Now, 'server_data' and 'user_data' contain the final server and user data respectively\n",
        "# here the server and user data are both \"train\" data which are non overlapping\n",
        "#TODO: we will assign uid to the user_data now\n",
        "CHECKIN_PATH = './checkins_with_neighborhood_id_side.csv'\n",
        "checkins = pd.read_csv(CHECKIN_PATH)\n",
        "checkins.drop(['venueid', 'venue_category', 'count'], axis=1, inplace=True)\n",
        "checkins['utctime'] = pd.to_datetime(checkins['utctime']).dt.tz_localize(None)\n",
        "checkins.rename(columns={\"utctime\": \"time\"}, inplace=True)\n",
        "checkins.sort_values(by=['time'], inplace=True)\n",
        "\n",
        "# client_list_500 = list(range(1, 501))  # Creates a list from 1 to 500 (inclusive)\n",
        "# np.random.shuffle(client_list_500)\n",
        "\n",
        "def assign_uid_to_crime():\n",
        "    for i in range(1,10):\n",
        "        crimes_i = user_data[user_data['neighbourhood_id'] == i]\n",
        "        checkins_i = checkins[(checkins['neighbourhood_id'] == i)]\n",
        "        START_DATE = datetime.strptime('04/01/2012 00:00:00', '%m/%d/%Y %H:%M:%S')  \n",
        "        END_DATE = datetime.strptime('12/31/2012 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "        checkins_i = checkins_i[checkins_i['time'] >=START_DATE]\n",
        "        checkins_i = checkins_i[checkins_i['time'] <=END_DATE]\n",
        "        \n",
        "        print(f\"region:  {i}, crimes :{crimes_i.shape[0]}, checkins: {checkins_i.shape[0]}, uid: {checkins_i['userid'].unique().shape[0]}\")\n",
        "        if(checkins_i['userid'].unique().shape[0] > 0):\n",
        "            # Shuffle the array to ensure randomness\n",
        "            np.random.shuffle(checkins_i['userid'].unique())\n",
        "\n",
        "            # Take the first cluster_head_count values\n",
        "            unique_user_idssss = checkins_i['userid'].unique()\n",
        "            # print('unique user iddss ',len(unique_user_idssss))\n",
        "            actual_unique_user_length = len(unique_user_idssss)\n",
        "            # print(f'len of actual uniq users for ${i} ',actual_unique_user_length)\n",
        "            # selected_uids = set()\n",
        "            index_count = 0\n",
        "            for index, row in crimes_i.iterrows():\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid']))\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(checkins_i['userid'].unique()[:cluster_head_count]))\n",
        "                # user_data.loc[index, 'uid'] = int(np.random.choice(20))\n",
        "                # Ensure that we loop back to the beginning of the array if we reach its end\n",
        "                uid = int(unique_user_idssss[index_count % actual_unique_user_length])\n",
        "                # selected_uids.add(uid)\n",
        "                user_data.loc[index, 'uid'] = uid\n",
        "                index_count+=1\n",
        "            \n",
        "            # print(f'selected uid for region {i} is ',len(selected_uids))\n",
        "        else :\n",
        "            for index, row in crimes_i.iterrows():\n",
        "                user_data.loc[index, 'uid'] = -1\n",
        "        # unique_user_idssss = client_list_500[:cluster_head_count]\n",
        "        # unique_user_idssss = checkins_i['userid'].unique()[:cluster_head_count]\n",
        "        # actual_unique_user_length = len(unique_user_idssss)\n",
        "        # for index, row in crimes_i.iterrows():\n",
        "        #     uid = int(unique_user_idssss[index % actual_unique_user_length])\n",
        "        #     user_data.loc[index, 'uid'] = uid\n",
        "    print('uid assignment for all the 77 regions done,but there are -1 uid also so we have to replace them with those uids which have very low crime incidents')\n",
        "    uid_row_count = user_data['uid'].value_counts().reset_index()\n",
        "    uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "    # Sort the DataFrame by 'uid'\n",
        "    uid_row_count = uid_row_count.sort_values(by='crime_count')\n",
        "\n",
        "\n",
        "    # Get the top 1946 uid values\n",
        "    top_uid_list = uid_row_count.head(1946)['uid'].tolist()\n",
        "    # print('top uid list ',top_uid_list)\n",
        "\n",
        "    # Read the DataFrame where uid == -1\n",
        "    # df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "\n",
        "    # # Assign a randomly chosen uid from the top 1946 uid list\n",
        "    # df_uid_minus_one['uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "    # df_uid_minus_one['uid'] = df_uid_minus_one['uid'].astype(int)\n",
        "    \n",
        "    # Use .loc to update the DataFrame where uid == -1\n",
        "    df_uid_minus_one = user_data[user_data['uid'] == -1]\n",
        "    user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=True)\n",
        "    # user_data.loc[user_data['uid'] == -1, 'uid'] = np.random.choice(top_uid_list, size=len(df_uid_minus_one), replace=False)\n",
        "    user_data['uid'] = user_data['uid'].astype(int)\n",
        "\n",
        "    print('--------------- done with replacing -1 with the random uid which has very less data (random from bottom, around 1946 unique uids which has very very less crime data)')\n",
        "    print(\"######### ------------- All Crimes have been assigned with a User ------------- ##################\")\n",
        "    \n",
        "\n",
        "assign_uid_to_crime()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  K-Means Clustering for checkin based clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n",
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1429: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15188\\2937629579.py:80: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------- user data and server data saved in csv -------------\n",
            "-------------user crime count also stored -------------------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "def K_Means_Clustering(region):\n",
        "    user_vectors = {}\n",
        "    # Create a dictionary to store the clusters with unique IDs for each region\n",
        "    region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "\n",
        "    checkins_region = checkins[checkins['neighbourhood_id'] == region]\n",
        "    START_DATE = datetime.strptime('04/01/2012 00:00:00', '%m/%d/%Y %H:%M:%S')  \n",
        "    END_DATE = datetime.strptime('12/31/2012 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "    checkins_region = checkins_region[checkins_region['time'] >=START_DATE]\n",
        "    checkins_region = checkins_region[checkins_region['time'] <=END_DATE]\n",
        "\n",
        "    clusters = {}\n",
        "    if checkins_region.shape[0] > 0:\n",
        "        clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "        user_ids = checkins_region['userid'].unique()\n",
        "        # print(f\"userid list for region {region}->{user_ids}\")\n",
        "        for user_id in user_ids:\n",
        "            user_vectors[user_id] = [0] * 9  # Initialize vector of size 9 with zeros\n",
        "\n",
        "        for user_id in user_ids:\n",
        "            user_checkins = checkins_region[checkins_region['userid'] == user_id]\n",
        "            for month in range(4, 13):  # From April (4) to December (12)\n",
        "                start_date = pd.Timestamp(year=2012, month=month, day=1)\n",
        "                if month == 12:\n",
        "                    end_date = pd.Timestamp(year=2012, month=month, day=31, hour=23, minute=59, second=59)\n",
        "                else:\n",
        "                    end_date = pd.Timestamp(year=2012, month=month+1, day=1) - pd.Timedelta(seconds=1)\n",
        "                \n",
        "                has_checkin = user_checkins[(user_checkins['time'] >= start_date) & (user_checkins['time'] <= end_date)].shape[0] > 0\n",
        "                user_vectors[user_id][month-4] = 1 if has_checkin else 0  # 1 if user has a checkin in the month, 0 otherwise\n",
        "                # start_date = pd.Timestamp(year=2013, month=month, day=1)\n",
        "                # if month == 12:\n",
        "                #     end_date = pd.Timestamp(year=2013, month=month, day=31, hour=23, minute=59, second=59)\n",
        "                # else:\n",
        "                #     end_date = pd.Timestamp(year=2013, month=month+1, day=1) - pd.Timedelta(seconds=1)\n",
        "                \n",
        "                # has_checkin = user_checkins[(user_checkins['time'] >= start_date) & (user_checkins['time'] <= end_date)].shape[0] > 0\n",
        "                # user_vectors[user_id][month-4] = 1 if has_checkin else 0  # 1 if user has a checkin in the month, 0 otherwise\n",
        "            if(np.sum(user_vectors[user_id])==0):\n",
        "                print(f\"user_id -{user_id}:{user_vectors[user_id]}\")\n",
        "                print(\"Error found\\n\")\n",
        "            \n",
        "    \n",
        "        # Convert user_vectors dictionary to a list of vectors\n",
        "        user_ids = list(user_vectors.keys())\n",
        "        vectors = list(user_vectors.values())\n",
        "\n",
        "        possible_cluster_head_count = min(cluster_head_count,len(user_ids))\n",
        "\n",
        "        # Perform K-means clustering\n",
        "        kmeans = KMeans(n_clusters=possible_cluster_head_count, random_state=0).fit(preprocessing.normalize(vectors)) # for cosine similarity normalize is done\n",
        "\n",
        "        # Create a dictionary to store the clusters with unique IDs for each region\n",
        "        # region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "        # clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "\n",
        "        # Assign user_ids to their respective clusters\n",
        "        for idx, label in enumerate(kmeans.labels_):\n",
        "            clusters[label + region_offset].append(user_ids[idx])\n",
        "\n",
        "       \n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a counter for the total number of CSV files created\n",
        "# Create an empty DataFrame to store the K-means clustered user data\n",
        "k_means_clustered_user_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "for region in range(1, 10):\n",
        "    cluster_results = K_Means_Clustering(region)\n",
        "    for cluster_id, user_ids in cluster_results.items():\n",
        "        # Filter user_data for the current cluster\n",
        "        cluster_data = user_data[user_data['uid'].isin(user_ids)]\n",
        "\n",
        "        # Set the uid as the cluster_id for all the data in this cluster\n",
        "        cluster_data['uid'] = cluster_id\n",
        "\n",
        "        # Append the cluster data to the k_means_clustered_user_data DataFrame\n",
        "        k_means_clustered_user_data = pd.concat([k_means_clustered_user_data, cluster_data], ignore_index=True)\n",
        "        \n",
        "    \n",
        "    \n",
        "\n",
        "server_data['neighbourhood_id'] = server_data['neighbourhood_id'].astype(int)\n",
        "server_data.to_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv', index=False)\n",
        "k_means_clustered_user_data.to_csv(f'./crimes_with_uid_{save_file_path}.csv', index=False)\n",
        "print('----------- user data and server data saved in csv -------------')\n",
        "\n",
        "uid_row_count = k_means_clustered_user_data['uid'].value_counts().reset_index()\n",
        "uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "uid_row_count = uid_row_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "uid_row_count.to_csv(f'user_crimes_count_{save_file_path}.csv', index=False)\n",
        "print('-------------user crime count also stored -------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HP\\.conda\\envs\\fedcrime\\Lib\\site-packages\\tslearn\\bases\\bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
            "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
            "  warn(h5py_msg)\n",
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4372\\1170243191.py:77: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created  cluster CSV files for region 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_4372\\1170243191.py:77: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  cluster_data['uid'] = cluster_id\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully created  cluster CSV files for region 25\n",
            "----------- user data and server data saved in csv -------------\n",
            "-------------user crime count also stored -------------------------\n"
          ]
        }
      ],
      "source": [
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.metrics import dtw\n",
        "\n",
        "def K_Means_Clustering(user_data,region):\n",
        "    user_vectors = {}\n",
        "    # Create a dictionary to store the clusters with unique IDs for each region\n",
        "    region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "\n",
        "    checkins_region = checkins[checkins['neighbourhood_id'] == region]\n",
        "    \n",
        "    clusters = {}\n",
        "    if checkins_region.shape[0] > 0:\n",
        "        clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "        user_ids = checkins_region['userid'].unique()\n",
        "        for user_id in user_ids:\n",
        "            user_vectors[user_id] = [0] * 9  # Initialize vector of size 9 with zeros\n",
        "\n",
        "        for user_id in user_ids:\n",
        "            user_checkins = checkins_region[checkins_region['userid'] == user_id]\n",
        "            for month in range(4, 13):  # From April (4) to December (12)\n",
        "                start_date = pd.Timestamp(year=2012, month=month, day=1)\n",
        "                if month == 12:\n",
        "                    end_date = pd.Timestamp(year=2012, month=month, day=31, hour=23, minute=59, second=59)\n",
        "                else:\n",
        "                    end_date = pd.Timestamp(year=2012, month=month+1, day=1) - pd.Timedelta(seconds=1)\n",
        "                \n",
        "                has_checkin = user_checkins[(user_checkins['time'] >= start_date) & (user_checkins['time'] <= end_date)].shape[0] > 0\n",
        "                user_vectors[user_id][month-4] = 1 if has_checkin else 0  # 1 if user has a checkin in the month, 0 otherwise\n",
        "    \n",
        "    \n",
        "        # Convert user_vectors dictionary to a list of vectors\n",
        "        user_ids = list(user_vectors.keys())\n",
        "        vectors = list(user_vectors.values())\n",
        "\n",
        "        possible_cluster_head_count = min(cluster_head_count,len(user_ids))\n",
        "\n",
        "        # Perform K-means clustering\n",
        "        kmeans = TimeSeriesKMeans(n_clusters=possible_cluster_head_count, metric='dtw', random_state=0).fit(vectors)\n",
        "\n",
        "        # Create a dictionary to store the clusters with unique IDs for each region\n",
        "        # region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "        # clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "\n",
        "        # Assign user_ids to their respective clusters\n",
        "        for idx, label in enumerate(kmeans.labels_):\n",
        "            clusters[label + region_offset].append(user_ids[idx])\n",
        "\n",
        "        # Create a dictionary to store the merged vectors for each cluster\n",
        "        # merged_clusters = {(i + region_offset): [0] * 9 for i in range(cluster_head_count)}\n",
        "\n",
        "        # # Merge vectors for each cluster\n",
        "        # for cluster_id, users in clusters.items():\n",
        "        #     for user_id in users:\n",
        "        #         user_vector = user_vectors[user_id]\n",
        "        #         merged_clusters[cluster_id] = [sum(x) for x in zip(merged_clusters[cluster_id], user_vector)]\n",
        "\n",
        "        # Print the merged clusters\n",
        "        # for cluster_id, merged_vector in merged_clusters.items():\n",
        "        #     print(f\"Region {region}, Cluster {cluster_id}: {merged_vector}\")\n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a counter for the total number of CSV files created\n",
        "# Create an empty DataFrame to store the K-means clustered user data\n",
        "k_means_clustered_user_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "for region in range(1,10):\n",
        "    cluster_results = K_Means_Clustering(user_data, region)\n",
        "    for cluster_id, user_ids in cluster_results.items():\n",
        "        # Filter user_data for the current cluster\n",
        "        cluster_data = user_data[user_data['uid'].isin(user_ids)]\n",
        "\n",
        "        # Set the uid as the cluster_id for all the data in this cluster\n",
        "        cluster_data['uid'] = cluster_id\n",
        "\n",
        "        # Append the cluster data to the k_means_clustered_user_data DataFrame\n",
        "        k_means_clustered_user_data = pd.concat([k_means_clustered_user_data, cluster_data], ignore_index=True)\n",
        "        \n",
        "    \n",
        "    # Assert that we've created the correct number of CSV files for this region\n",
        "    # assert total_csv_files == len(cluster_results), \\\n",
        "    #     f\"Expected {len(cluster_results)} CSV files, but created {total_csv_files}\"\n",
        "\n",
        "    print(f\"Successfully created  cluster CSV files for region {region}\")\n",
        "\n",
        "server_data['neighbourhood_id'] = server_data['neighbourhood_id'].astype(int)\n",
        "server_data.to_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv', index=False)\n",
        "k_means_clustered_user_data.to_csv(f'./crimes_with_uid_{save_file_path}.csv', index=False)\n",
        "print('----------- user data and server data saved in csv -------------')\n",
        "\n",
        "uid_row_count = k_means_clustered_user_data['uid'].value_counts().reset_index()\n",
        "uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "uid_row_count = uid_row_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "uid_row_count.to_csv(f'user_crimes_count_{save_file_path}.csv', index=False)\n",
        "print('-------------user crime count also stored -------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means Clustering for user crime count based clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def K_Means_Clustering(user_data,region):\n",
        "    user_vectors = {}\n",
        "    # Create a dictionary to store the clusters with unique IDs for each region\n",
        "    region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "\n",
        "    user_data_region = user_data[user_data['neighbourhood_id'] == region]\n",
        "    \n",
        "    clusters = {}\n",
        "    if user_data_region.shape[0] > 0:\n",
        "        clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "        user_ids = user_data_region['uid'].unique()\n",
        "        for user_id in user_ids:\n",
        "            user_vectors[user_id] = [0] * 9  # Initialize vector of size 9 with zeros\n",
        "\n",
        "        for user_id in user_ids:\n",
        "            user_crimes = user_data_region[user_data_region['uid'] == user_id]\n",
        "            for month in range(4, 13):  # From April (4) to December (12)\n",
        "                start_date = pd.Timestamp(year=2019, month=month, day=1)\n",
        "                if month == 12:\n",
        "                    end_date = pd.Timestamp(year=2019, month=month, day=31, hour=23, minute=59, second=59)\n",
        "                else:\n",
        "                    end_date = pd.Timestamp(year=2019, month=month+1, day=1) - pd.Timedelta(seconds=1)\n",
        "                \n",
        "                crime_count = user_crimes[(user_crimes['time'] >= start_date) & (user_crimes['time'] <= end_date)].shape[0]\n",
        "                user_vectors[user_id][month-4] = crime_count  # Store the crime count for each month\n",
        "    \n",
        "    \n",
        "        # Convert user_vectors dictionary to a list of vectors\n",
        "        user_ids = list(user_vectors.keys())\n",
        "        vectors = list(user_vectors.values())\n",
        "\n",
        "        possible_cluster_head_count = min(cluster_head_count,len(user_ids))\n",
        "\n",
        "        # Perform K-means clustering\n",
        "        kmeans = KMeans(n_clusters=possible_cluster_head_count, random_state=0).fit(vectors)\n",
        "\n",
        "        # Create a dictionary to store the clusters with unique IDs for each region\n",
        "        # region_offset = region * 1000  # Assuming max 1000 clusters per region\n",
        "        # clusters = {(i + region_offset): [] for i in range(cluster_head_count)}\n",
        "\n",
        "        # Assign user_ids to their respective clusters\n",
        "        for idx, label in enumerate(kmeans.labels_):\n",
        "            clusters[label + region_offset].append(user_ids[idx])\n",
        "\n",
        "        # Create a dictionary to store the merged vectors for each cluster\n",
        "        # merged_clusters = {(i + region_offset): [0] * 9 for i in range(cluster_head_count)}\n",
        "\n",
        "        # # Merge vectors for each cluster\n",
        "        # for cluster_id, users in clusters.items():\n",
        "        #     for user_id in users:\n",
        "        #         user_vector = user_vectors[user_id]\n",
        "        #         merged_clusters[cluster_id] = [sum(x) for x in zip(merged_clusters[cluster_id], user_vector)]\n",
        "\n",
        "        # Print the merged clusters\n",
        "        # for cluster_id, merged_vector in merged_clusters.items():\n",
        "        #     print(f\"Region {region}, Cluster {cluster_id}: {merged_vector}\")\n",
        "\n",
        "    return clusters\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a counter for the total number of CSV files created\n",
        "# Create an empty DataFrame to store the K-means clustered user data\n",
        "k_means_clustered_user_data = pd.DataFrame()\n",
        "\n",
        "\n",
        "for region in range(1, 78):\n",
        "    cluster_results = K_Means_Clustering(user_data, region)\n",
        "    for cluster_id, user_ids in cluster_results.items():\n",
        "        # Filter user_data for the current cluster\n",
        "        cluster_data = user_data[user_data['uid'].isin(user_ids)]\n",
        "\n",
        "        # Set the uid as the cluster_id for all the data in this cluster\n",
        "        cluster_data['uid'] = cluster_id\n",
        "\n",
        "        # Append the cluster data to the k_means_clustered_user_data DataFrame\n",
        "        k_means_clustered_user_data = pd.concat([k_means_clustered_user_data, cluster_data], ignore_index=True)\n",
        "        \n",
        "    \n",
        "    # Assert that we've created the correct number of CSV files for this region\n",
        "    # assert total_csv_files == len(cluster_results), \\\n",
        "    #     f\"Expected {len(cluster_results)} CSV files, but created {total_csv_files}\"\n",
        "\n",
        "    # print(f\"Successfully created {total_csv_files} cluster CSV files for region {region}\")\n",
        "\n",
        "server_data['neighbourhood_id'] = server_data['neighbourhood_id'].astype(int)\n",
        "server_data.to_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv', index=False)\n",
        "k_means_clustered_user_data.to_csv(f'./crimes_with_uid_{save_file_path}.csv', index=False)\n",
        "print('----------- user data and server data saved in csv -------------')\n",
        "\n",
        "uid_row_count = k_means_clustered_user_data['uid'].value_counts().reset_index()\n",
        "uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "uid_row_count = uid_row_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "uid_row_count.to_csv(f'user_crimes_count_{save_file_path}.csv', index=False)\n",
        "print('-------------user crime count also stored -------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9553Dn1PGkB",
        "outputId": "0bc2ab25-fdbe-4449-cbc0-9fb7e9034014"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Set the seed\n",
        "seed_value = 46\n",
        "np.random.seed(seed_value)\n",
        "assign_uid_to_crime()\n",
        "print(user_data.head(3))\n",
        "print('-------------- uid assignment done --------------------')\n",
        "\n",
        "# ------------------ uid assign done -------------------------\n",
        "# Step 3: Isolate a certain portion of uid (isolated_users_percentage) with no data shared with the server\n",
        "# user_data['uid'] = user_data['uid'].astype(int)\n",
        "unique_uids = user_data['uid'].unique()\n",
        "isolated_uids = np.random.choice(unique_uids, size=int(isolated_users_percentage * len(unique_uids)), replace=False)\n",
        "isolated_uids = isolated_uids.astype(int)\n",
        "# isolated_user_data = user_data[user_data['uid'].isin(isolated_uids)]\n",
        "\n",
        "# Step 4: Generate new overlapping user data\n",
        "\n",
        "#make sure that the neighbourhood_id is integer\n",
        "server_data['neighbourhood_id'] = server_data['neighbourhood_id'].astype(int)\n",
        "\n",
        "new_overlapping_user_data = pd.DataFrame()\n",
        "\n",
        "if(overlapping_percentage != 0):\n",
        "    # Iterate through each group for server data\n",
        "    for group_name, group_data in server_data.groupby('neighbourhood_id'):\n",
        "        # Extract overlapping_percentage percentage of random server data\n",
        "        extracted_group_data = group_data.sample(frac=overlapping_percentage, random_state=42)\n",
        "\n",
        "        # Get the unique uid for the current group\n",
        "        unique_uids_for_group = user_data[user_data['neighbourhood_id'] == group_name]['uid'].unique()\n",
        "\n",
        "        # Assign a random uid from the remaining uids to each data point in the extracted group data\n",
        "        remaining_uids = np.setdiff1d(unique_uids_for_group, isolated_uids)\n",
        "        if remaining_uids.size != 0:\n",
        "            extracted_group_data['uid'] = np.random.choice(remaining_uids, size=len(extracted_group_data), replace=True)\n",
        "            extracted_group_data['uid'] = extracted_group_data['uid'].astype(int)\n",
        "\n",
        "        # Concatenate with the new overlapping user data\n",
        "        new_overlapping_user_data = pd.concat([new_overlapping_user_data, extracted_group_data])\n",
        "\n",
        "    # Concatenate the new overlapping user data with the isolated user data to get the final user data\n",
        "    user_data = pd.concat([user_data, new_overlapping_user_data])\n",
        "\n",
        "# Now, 'server_data' and 'user_data' contain the final server and user data respectively\n",
        "server_data.to_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv', index=False)\n",
        "user_data.to_csv(f'./crimes_with_uid_{save_file_path}.csv', index=False)\n",
        "print('----------- user data and server data saved in csv -------------')\n",
        "\n",
        "uid_row_count = user_data['uid'].value_counts().reset_index()\n",
        "uid_row_count.columns = ['uid', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "uid_row_count = uid_row_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "uid_row_count.to_csv(f'user_crimes_count_{save_file_path}.csv', index=False)\n",
        "print('-------------user crime count also stored -------------------------')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "t3fH-CeiPGkC",
        "outputId": "a9a53dfc-d651-4418-b5bc-f5e22f5773c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------region crime count also stored -------------------------\n"
          ]
        }
      ],
      "source": [
        "# getting the region wise crime list to sort which region has greater crime incidents\n",
        "user_data_for_new_calc = pd.read_csv(f'./crimes_with_uid_{save_file_path}.csv')\n",
        "region_crime_count = user_data_for_new_calc['neighbourhood_id'].value_counts().reset_index()\n",
        "region_crime_count.columns = ['region_id', 'crime_count']\n",
        "\n",
        "# Sort the DataFrame by 'uid'\n",
        "region_crime_count = region_crime_count.sort_values(by='crime_count', ascending=False)\n",
        "# Store the results in a new CSV file\n",
        "region_crime_count.to_csv(f'region_crime_counts_{save_file_path}.csv', index=False)\n",
        "print('-------------region crime count also stored -------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0ZGFoELAPGkD",
        "outputId": "ca222d27-95e1-4995-d3a8-da171518515c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "225\n"
          ]
        }
      ],
      "source": [
        "corresponding_uids_data = pd.read_csv(root_folder+f'/user_crimes_count_{save_file_path}.csv')\n",
        "corresponding_uids = corresponding_uids_data['uid'].to_numpy()\n",
        "print(len(corresponding_uids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsSiDXpdPGkD"
      },
      "source": [
        "#### User Train Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Kwk7kj71PGkE",
        "outputId": "203386ba-8874-47e5-8647-df335425dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user id completed :  5010\n",
            "user id completed :  3003\n",
            "user id completed :  4010\n",
            "user id completed :  5013\n",
            "user id completed :  5020\n",
            "user id completed :  5004\n",
            "user id completed :  4003\n",
            "user id completed :  4002\n",
            "user id completed :  4006\n",
            "user id completed :  4004\n",
            "user id completed :  7001\n",
            "user id completed :  6006\n",
            "user id completed :  6007\n",
            "user id completed :  5003\n",
            "user id completed :  3019\n",
            "user id completed :  5001\n",
            "user id completed :  3002\n",
            "user id completed :  3010\n",
            "user id completed :  4005\n",
            "user id completed :  6001\n",
            "user id completed :  7000\n",
            "user id completed :  7007\n",
            "user id completed :  3006\n",
            "user id completed :  4000\n",
            "user id completed :  4015\n",
            "user id completed :  5008\n",
            "user id completed :  7002\n",
            "user id completed :  3013\n",
            "user id completed :  3021\n",
            "user id completed :  5016\n",
            "user id completed :  5006\n",
            "user id completed :  4008\n",
            "user id completed :  5011\n",
            "user id completed :  3007\n",
            "user id completed :  6008\n",
            "user id completed :  9001\n",
            "user id completed :  1006\n",
            "user id completed :  7005\n",
            "user id completed :  1007\n",
            "user id completed :  4014\n",
            "user id completed :  5021\n",
            "user id completed :  1002\n",
            "user id completed :  5002\n",
            "user id completed :  1000\n",
            "user id completed :  4011\n",
            "user id completed :  1012\n",
            "user id completed :  5015\n",
            "user id completed :  6009\n",
            "user id completed :  4021\n",
            "user id completed :  3005\n",
            "user id completed :  4012\n",
            "user id completed :  6000\n",
            "user id completed :  3004\n",
            "user id completed :  9006\n",
            "user id completed :  1004\n",
            "user id completed :  5024\n",
            "user id completed :  4013\n",
            "user id completed :  5007\n",
            "user id completed :  3018\n",
            "user id completed :  7011\n",
            "user id completed :  6011\n",
            "user id completed :  7004\n",
            "user id completed :  4007\n",
            "user id completed :  5019\n",
            "user id completed :  3008\n",
            "user id completed :  9003\n",
            "user id completed :  7006\n",
            "user id completed :  3023\n",
            "user id completed :  1003\n",
            "user id completed :  3012\n",
            "user id completed :  5023\n",
            "user id completed :  2006\n",
            "user id completed :  5014\n",
            "user id completed :  4017\n",
            "user id completed :  1008\n",
            "user id completed :  9009\n",
            "user id completed :  1022\n",
            "user id completed :  8005\n",
            "user id completed :  1014\n",
            "user id completed :  5018\n",
            "user id completed :  4001\n",
            "user id completed :  1005\n",
            "user id completed :  5017\n",
            "user id completed :  5012\n",
            "user id completed :  2003\n",
            "user id completed :  8004\n",
            "user id completed :  2002\n",
            "user id completed :  3015\n",
            "user id completed :  9002\n",
            "user id completed :  6005\n",
            "user id completed :  3020\n",
            "user id completed :  3016\n",
            "user id completed :  6020\n",
            "user id completed :  9008\n",
            "user id completed :  2001\n",
            "user id completed :  9000\n",
            "user id completed :  4020\n",
            "user id completed :  6016\n",
            "user id completed :  8009\n",
            "user id completed :  6012\n",
            "user id completed :  7010\n",
            "user id completed :  3009\n",
            "user id completed :  3017\n",
            "user id completed :  3001\n",
            "user id completed :  4023\n",
            "user id completed :  5000\n",
            "user id completed :  3011\n",
            "user id completed :  6010\n",
            "user id completed :  2004\n",
            "user id completed :  4018\n",
            "user id completed :  6017\n",
            "user id completed :  6014\n",
            "user id completed :  3022\n",
            "user id completed :  5022\n",
            "user id completed :  3014\n",
            "user id completed :  7017\n",
            "user id completed :  5009\n",
            "user id completed :  8008\n",
            "user id completed :  1009\n",
            "user id completed :  4019\n",
            "user id completed :  7013\n",
            "user id completed :  8010\n",
            "user id completed :  8000\n",
            "user id completed :  1019\n",
            "user id completed :  6015\n",
            "user id completed :  9007\n",
            "user id completed :  4022\n",
            "user id completed :  6004\n",
            "user id completed :  7003\n",
            "user id completed :  8012\n",
            "user id completed :  2009\n",
            "user id completed :  1024\n",
            "user id completed :  8011\n",
            "user id completed :  4009\n",
            "user id completed :  7009\n",
            "user id completed :  6021\n",
            "user id completed :  9016\n",
            "user id completed :  1016\n",
            "user id completed :  1011\n",
            "user id completed :  9011\n",
            "user id completed :  7008\n",
            "user id completed :  1015\n",
            "user id completed :  1017\n",
            "user id completed :  3000\n",
            "user id completed :  7016\n",
            "user id completed :  7012\n",
            "user id completed :  4016\n",
            "user id completed :  6003\n",
            "user id completed :  2022\n",
            "user id completed :  9023\n",
            "user id completed :  6023\n",
            "user id completed :  1023\n",
            "user id completed :  8006\n",
            "user id completed :  1001\n",
            "user id completed :  1013\n",
            "user id completed :  9018\n",
            "user id completed :  2005\n",
            "user id completed :  8001\n",
            "user id completed :  7021\n",
            "user id completed :  3024\n",
            "user id completed :  6002\n",
            "user id completed :  1021\n",
            "user id completed :  6018\n",
            "user id completed :  4024\n",
            "user id completed :  6019\n",
            "user id completed :  2007\n",
            "user id completed :  2021\n",
            "user id completed :  2011\n",
            "user id completed :  6022\n",
            "user id completed :  2013\n",
            "user id completed :  1010\n",
            "user id completed :  5005\n",
            "user id completed :  8017\n",
            "user id completed :  1018\n",
            "user id completed :  7015\n",
            "user id completed :  7014\n",
            "user id completed :  1020\n",
            "user id completed :  9013\n",
            "user id completed :  9012\n",
            "user id completed :  7018\n",
            "user id completed :  9015\n",
            "user id completed :  7020\n",
            "user id completed :  2015\n",
            "user id completed :  8014\n",
            "user id completed :  7019\n",
            "user id completed :  2014\n",
            "user id completed :  2008\n",
            "user id completed :  8002\n",
            "user id completed :  2019\n",
            "user id completed :  2016\n",
            "user id completed :  7022\n",
            "user id completed :  9017\n",
            "user id completed :  2010\n",
            "user id completed :  7023\n",
            "user id completed :  2020\n",
            "user id completed :  2018\n",
            "user id completed :  8019\n",
            "user id completed :  9005\n",
            "user id completed :  7024\n",
            "user id completed :  8016\n",
            "user id completed :  2000\n",
            "user id completed :  8023\n",
            "user id completed :  9022\n",
            "user id completed :  9014\n",
            "user id completed :  8013\n",
            "user id completed :  2024\n",
            "user id completed :  8020\n",
            "user id completed :  2012\n",
            "user id completed :  9021\n",
            "user id completed :  8003\n",
            "user id completed :  9004\n",
            "user id completed :  2017\n",
            "user id completed :  9020\n",
            "user id completed :  2023\n",
            "user id completed :  8024\n",
            "user id completed :  8022\n",
            "user id completed :  9019\n",
            "user id completed :  8018\n",
            "user id completed :  6024\n",
            "user id completed :  8015\n",
            "user id completed :  8021\n",
            "user id completed :  8007\n",
            "user id completed :  9024\n",
            "user id completed :  9010\n",
            "user id completed :  6013\n"
          ]
        }
      ],
      "source": [
        "# # now we will create the side dataset based on data/chicago/side_com_adj.txt file\n",
        "'''from here we will need the rest of the codes,'''\n",
        "# from tqdm.notebook import tqdm, trange\n",
        "import shutil\n",
        "\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('09/19/2019 07:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "# read the data lat lon file\n",
        "# user_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "user_crime_full_df = pd.read_csv(f'./crimes_with_uid_{save_file_path}.csv')\n",
        "user_crime_full_df['time'] = pd.to_datetime(user_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# find the unique user ids from this df\n",
        "# unique_user_ids = user_crime_full_df['uid'].unique()\n",
        "#from all the unique user ids, take 1000 user_ids that are not present in the corresponding uids\n",
        "# Filter out 1000 uids that are not present in corr_uids\n",
        "# filtered_uids_for_server_data = [uid for uid in unique_user_ids if uid not in corresponding_uids][:2000]\n",
        "# get the rest uids for cross_mixing\n",
        "# cross_mixing_uids = [uid for uid in unique_user_ids if uid not in filtered_uids_for_server_data and uid not in corresponding_uids]\n",
        "\n",
        "\n",
        "#now get the dataframe where the uids are the cross_mixing_uids\n",
        "# cross_mixing_df = user_crime_full_df[user_crime_full_df['uid'].isin(cross_mixing_uids)]\n",
        "\n",
        "# print('unique user ids are ',len(unique_user_ids))\n",
        "# sort this unique_user_ids in ascending order\n",
        "# unique_user_ids.sort()\n",
        "# taken_user_ids = 1015  # here many users are absent so we took 1015 to get approximately 1000 users\n",
        "# unique_user_ids = unique_user_ids[:taken_user_ids]\n",
        "# for each user id, create a directory in \"../data/chicago2/\" directory named that particular user id if the directory doesn't exist already\n",
        "# for id in trange(len(corresponding_uids)):\n",
        "for id in range(len(corresponding_uids)):\n",
        "    user_id = corresponding_uids[id]\n",
        "    # create the directory\n",
        "    # os.makedirs('/data/chicago/user/' + str(user_id), exist_ok=True)\n",
        "    # create a new df for each user id\n",
        "    user_crime_df = user_crime_full_df[user_crime_full_df['uid'] == user_id]\n",
        "\n",
        "    total_meighbors = 9\n",
        "    # total_days = 260\n",
        "    # total_days = 365  # calculted from START_DATE and END_DATE,not 260\n",
        "    # total_days = 261  # calculted from START_DATE and END_DATE,not 365, (END_DATE - START_DATE)  (excludes the END_DATE)\n",
        "    total_days = 275  # calculted from START_DATE and END_DATE,not 261, (END_DATE - START_DATE)  (includes the END_DATE)\n",
        "    time_division_per_day = 6 # here each day is divided in 12 hrs slot, so each day has 2hrs slot\n",
        "    hours_per_cell  = 4\n",
        "    total_cols = total_days * time_division_per_day\n",
        "    # now we have only first 8 hours (07:59:59) for the END_DATE, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "    # total_cols +=2\n",
        "    # total number of row is total crimes in crime_mapping\n",
        "    total_rows = len(crime_mapping)\n",
        "    # create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "    user_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "    '''code for adding the side crimes'''\n",
        "    # create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "    user_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "    # Convert the time format to datetime\n",
        "    # user_crime_df['time'] = pd.to_datetime(user_crime_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # run loop for each row\n",
        "    for index, row in user_crime_df.iterrows():\n",
        "        # get the neighbourhood id\n",
        "        neighbourhood_id = row['neighbourhood_id']\n",
        "        neighbourhood_id -=1 # for 0 based indexing\n",
        "        # get the crime type id\n",
        "        crime_type_id = row['TYPE_ID']\n",
        "        # get the date\n",
        "        date = row['time']\n",
        "        # get the time\n",
        "        time = date.time()\n",
        "        # get the day\n",
        "        day = date.date()\n",
        "        # get the time division\n",
        "        time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "        # get the day index\n",
        "        day_index = (day - START_DATE.date()).days\n",
        "        # get the time division index\n",
        "        time_division_index = day_index * time_division_per_day + time_division\n",
        "        # increment the value in the tensor\n",
        "        user_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1   # for com_crime calculations\n",
        "        cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "        user_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1   # for side crime calculations\n",
        "\n",
        "\n",
        "\n",
        "    #mixing with user data started\n",
        "    # random_sampling_percentage = 20\n",
        "    #now take random_sampling_percentage of dataframe from crime_mixing_df\n",
        "    # crime_mixing_df_for_current_user = cross_mixing_df.sample(frac=random_sampling_percentage/100)\n",
        "\n",
        "    # crime_mixing_df_for_current_user['time'] = pd.to_datetime(crime_mixing_df_for_current_user['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # run loop for each row\n",
        "    # for index, row in crime_mixing_df_for_current_user.iterrows():\n",
        "    #     # get the neighbourhood id\n",
        "    #     neighbourhood_id = row['neighbourhood_id']\n",
        "    #     neighbourhood_id -=1 # for 0 based indexing\n",
        "    #     # get the crime type id\n",
        "    #     crime_type_id = row['TYPE_ID']\n",
        "    #     # get the date\n",
        "    #     date = row['time']\n",
        "    #     # get the time\n",
        "    #     time = date.time()\n",
        "    #     # get the day\n",
        "    #     day = date.date()\n",
        "    #     # get the time division\n",
        "    #     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    #     # get the day index\n",
        "    #     day_index = (day - START_DATE.date()).days\n",
        "    #     # get the time division index\n",
        "    #     time_division_index = day_index * time_division_per_day + time_division\n",
        "    #     # increment the value in the tensor\n",
        "    #     user_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "    #     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    #     user_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "    output_directory = f'./toy_data/chicago/user_{save_file_path}/'+str(user_id)\n",
        "    if os.path.exists(output_directory):\n",
        "      shutil.rmtree(output_directory)\n",
        "\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "    for i in range(total_meighbors):\n",
        "        filename = f\"{output_directory}/r_{i}.txt\"\n",
        "        # Extract the 2D slice at index i\n",
        "        slice_2d = user_crime_tensor[i, :, :]\n",
        "\n",
        "        # Save the 2D slice as a text file\n",
        "        np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "    # now same for side data\n",
        "    for i in range(9):\n",
        "        filename = f\"{output_directory}/s_{i}.txt\"\n",
        "        # Extract the 2D slice at index i\n",
        "        slice_2d = user_side_crime_tensor[i, :, :]\n",
        "\n",
        "        # Save the 2D slice as a text file\n",
        "        np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "    print('user id completed : ',user_id)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXfF0A8mPGkF"
      },
      "source": [
        "### Server Train Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IzMAIhImPGkF",
        "outputId": "13207114-777a-4222-8840-5cf7b7a10ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here it ended\n"
          ]
        }
      ],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# dataset1 = pd.read_csv('training_crime_data_for_last_9_months.csv')\n",
        "# test_start_date = dataset1['time'].iloc[0]\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "server_crime_full_df = pd.read_csv(f'./server_data_for_FED_AIST_{save_file_path}.csv')\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 9\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 261 # counted from START_DATE and END_DATE,(END_DATE - START_DATE) (excluding the END_DATE) not 365\n",
        "total_days = 275 # counted from START_DATE and END_DATE,(END_DATE - START_DATE) (including the END_DATE) not 261\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# now we have only first 8 hours (07:59:59) for the END_DATE, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "# total_cols +=2\n",
        "# total number of row is total crimes in crime_mapping\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "server_crime_full_df['time'] = pd.to_datetime(server_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in final_server_dataset.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = f'./toy_data/chicago/server_{save_file_path}'\n",
        "if os.path.exists(output_directory):\n",
        "    shutil.rmtree(output_directory)\n",
        "\n",
        "os.makedirs(output_directory)\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here it ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk8Tl2UUPGkG"
      },
      "source": [
        "### Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DwMZoz6PGkG",
        "outputId": "23675464-7585-4efd-b9db-84d89cb08998"
      },
      "outputs": [],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('09/19/2019 08:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('03/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "test_crime_full_df = pd.read_csv('./testing_crime_data_for_first_3_months.csv')\n",
        "# test_crime_full_df = test_crime_full_df.dropna()\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 77\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 103 # counted from START_DATE and END_DATE,(END_DATE - START_DATE),\n",
        "total_days = 90 # counted from START_DATE and END_DATE,(END_DATE - START_DATE),\n",
        "#! (excluding the START_DATE, because the START_DATE has not the full 24 hrs, it has hours from 08:00:00) not 365\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# total number of row is total crimes in crime_mapping\n",
        "# now we have only last 16 hours (15:59:59 precisely) for the START_DATE because the time starts from 08:00:00, so we cannot take the whole day, rather we will take additional(16/hours_per_cell) = (16/4)= 4 columns extra\n",
        "# total_cols += 4\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "test_crime_full_df['time'] = pd.to_datetime(test_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in test_crime_full_df.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    #! here we are subtracting 8 hours from the day because the START_DATE begins with time 08:00:00, we want to map 08:00:00 to 00:00:00, so we are subtracting 8 hours from the time slot for each row\n",
        "    # date = row['time'] - timedelta(hours=8)\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = \"./toy_data/chicago/test\"\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here test data generation ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86A4oDXfPGkH"
      },
      "source": [
        "### Aggregated Data Generation For Central AIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqnVw4tdPGkH",
        "outputId": "c4499f7a-19da-4cb3-9157-21d86601b606"
      },
      "outputs": [],
      "source": [
        "# now time to write the server code ,same code as the user code, just a single folder server folder will be created inside the data/chicago2 folder and all the 77 r txts and 9 s txts will be written\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "crime_mapping = {\n",
        "    'DECEPTIVE PRACTICE': 0,\n",
        "    'THEFT': 1,\n",
        "    'CRIMINAL DAMAGE': 2,\n",
        "    'BATTERY': 3,\n",
        "    'ROBBERY': 4,\n",
        "    'ASSAULT': 5,\n",
        "    'BURGLARY': 6,\n",
        "    'NARCOTICS': 7\n",
        "    }\n",
        "\n",
        "side_neighbourhood_dict = {}\n",
        "with open('./side_com_adj.txt', 'r') as f:\n",
        "    # read the lines\n",
        "    lines = f.readlines()\n",
        "    # now here each line has 2 integers, 1st is side_id and 2nd is neighbourhood_id\n",
        "    # create a dictionary, where key is neighbourhood_id and value is the side_id\n",
        "    for line in lines:\n",
        "        # split the line by space\n",
        "        line = line.split()\n",
        "        # get the side_id\n",
        "        side_id = int(line[0])\n",
        "        # get the neighbourhood_id\n",
        "        neighbourhood_id = int(line[1])\n",
        "        # modulus the side_id by 101\n",
        "        side_id = side_id % 101\n",
        "        # add the side_id to the dictionary\n",
        "        side_neighbourhood_dict[neighbourhood_id-1] = side_id  # here 1 is reduced because id starts from 1, making it 0 based indexing\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# START_DATE = datetime.strptime('01/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('09/19/2019 07:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "START_DATE = datetime.strptime('04/01/2019 00:00:00', '%m/%d/%Y %H:%M:%S')\n",
        "END_DATE = datetime.strptime('12/31/2019 23:59:59', '%m/%d/%Y %H:%M:%S')\n",
        "\n",
        "# print the number of days between this two dates including the start date\n",
        "# print((END_DATE - START_DATE).days + 1)\n",
        "\n",
        "# START_DATE = datetime.strptime('2012-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# END_DATE = datetime.strptime('2013-04-3 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# read the server short2 crime file\n",
        "# server_crime_full_df = pd.read_csv('/content/drive/MyDrive/Undergrad_thesis_final/AIST_UCA/data_latlon_xy_with_neighbourhood_id.csv')\n",
        "# full_crime_full_df = pd.read_csv('./training_crime_data_for_last_9_months.csv')\n",
        "full_crime_full_df = pd.read_csv('./Final_Training_Data.csv')\n",
        "# full_crime_full_df = full_crime_full_df.dropna()\n",
        "full_crime_full_df.rename(columns={\"Community Area\": \"neighbourhood_id\"}, inplace=True)\n",
        "\n",
        "# now filter the server crime full df so that it has uid only the filtered_uids_for_server_data\n",
        "# server_crime_full_df = server_crime_full_df[server_crime_full_df['uid'].isin(filtered_uids_for_server_data)]\n",
        "\n",
        "#take 90% data of the cross_mixing_df for server_cross_mixing_df\n",
        "# server_cross_mixing_df = cross_mixing_df.sample(frac=90/100)\n",
        "\n",
        "#rename the Date to time\n",
        "# server_crime_full_df.rename(columns={'Date':'time'}, inplace=True)\n",
        "# os.makedirs('/data/chicago2/server', exist_ok=True)\n",
        "\n",
        "# portion_to_take_from_each_region = 0.7\n",
        "\n",
        "# List of unique neighborhood IDs in the dataset\n",
        "# unique_neighborhoods = server_crime_full_df['neighbourhood_id'].unique()\n",
        "\n",
        "# Columns to include in the final dataset\n",
        "# columns_to_include = ['time', 'IUCR', 'Primary Type', 'neighbourhood_id', 'lat', 'lon', 'TYPE_ID', 'uid']\n",
        "\n",
        "# Initialize an empty DataFrame to store the final result\n",
        "# final_server_dataset = pd.DataFrame(columns=columns_to_include)\n",
        "\n",
        "# Loop through each neighborhood ID\n",
        "# for neighborhood_id in unique_neighborhoods:\n",
        "#     # Filter the data for the current neighborhood\n",
        "#     neighborhood_data = server_crime_full_df[server_crime_full_df['neighbourhood_id'] == neighborhood_id]\n",
        "\n",
        "#     # Calculate the number of samples to include (70%)\n",
        "#     num_samples = int(len(neighborhood_data) * portion_to_take_from_each_region)\n",
        "\n",
        "#     # Take the first 70% of the data\n",
        "#     neighborhood_train_data = neighborhood_data.head(num_samples)\n",
        "#     # print('neighbouhood train data type ',type(neighborhood_train_data))\n",
        "\n",
        "#     # Verify that exactly 70% of the data is included\n",
        "#     assert len(neighborhood_train_data) == num_samples, f\"Verification failed for neighborhood_id {neighborhood_id}\"\n",
        "#     # print('type after including columns ',type(neighborhood_train_data[columns_to_include]))\n",
        "\n",
        "#     # Append only the desired columns to the final dataset\n",
        "#     # final_server_dataset = final_server_dataset.append(neighborhood_train_data[columns_to_include], ignore_index=True)\n",
        "#     # Concatenate the neighborhood_train_data to the final dataset\n",
        "#     final_server_dataset = pd.concat([final_server_dataset, neighborhood_train_data[columns_to_include]], ignore_index=True)\n",
        "\n",
        "total_meighbors = 77\n",
        "# total_days = 260\n",
        "# total_days = 365 # counted from START_DATE and END_DATE, not 260\n",
        "# total_days = 261 # counted from START_DATE and END_DATE,(END_DATE - START_DATE), excluding the END_DATE because we dont have whole 24 hours for the END_DATE not 365\n",
        "total_days = 275 # counted from START_DATE and END_DATE,(END_DATE - START_DATE), including the END_DATE because we dont have whole 24 hours for the END_DATE not 365\n",
        "time_division_per_day = 6\n",
        "hours_per_cell  = 4\n",
        "total_cols = total_days * time_division_per_day\n",
        "# now we have only first 8 hours (07:59:59) for the last date, so we cannot take the whole day, rather we will take additional(8/hours_per_cell) = (8/4)= 2 columns extra\n",
        "# total_cols +=2\n",
        "# total number of row is total crimes in crime_mapping\n",
        "total_rows = len(crime_mapping)\n",
        "# create a 3d tensor of dimension total_regions X crimes X total_cols\n",
        "server_crime_tensor = np.zeros((total_meighbors, total_rows, total_cols))\n",
        "\n",
        "'''code for adding the side crimes'''\n",
        "# create a user side crime tensor of dimenstion 9 X crimes X total_cols\n",
        "server_side_crime_tensor = np.zeros((9, total_rows, total_cols))\n",
        "\n",
        "# Convert the time format to datetime\n",
        "full_crime_full_df['time'] = pd.to_datetime(full_crime_full_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# final_server_dataset = server_crime_full_df[(server_crime_full_df['time'] >= START_DATE) & (server_crime_full_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "for index, row in full_crime_full_df.iterrows():\n",
        "    # get the neighbourhood id\n",
        "    neighbourhood_id = row['neighbourhood_id']\n",
        "    neighbourhood_id -=1 # for 0 based indexing\n",
        "    # get the crime type id\n",
        "    crime_type_id = row['TYPE_ID']\n",
        "    # get the date\n",
        "    date = row['time']\n",
        "    # get the time\n",
        "    time = date.time()\n",
        "    # get the day\n",
        "    day = date.date()\n",
        "    # get the time division\n",
        "    time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "    # get the day index\n",
        "    day_index = (day - START_DATE.date()).days\n",
        "    # get the time division index\n",
        "    time_division_index = day_index * time_division_per_day + time_division\n",
        "    # increment the value in the tensor\n",
        "    server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1    # for com_crime calculations\n",
        "    cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "    server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1    # for side_crime calculations\n",
        "\n",
        "# Convert the time format to datetime\n",
        "# server_cross_mixing_df['time'] = pd.to_datetime(server_cross_mixing_df['time'], format='%Y-%m-%d %H:%M:%S')\n",
        "# filter the df to have data from start date to 365 days\n",
        "# server_cross_mixing_df = server_cross_mixing_df[(server_cross_mixing_df['time'] >= START_DATE) & (server_cross_mixing_df['time'] <= END_DATE)]\n",
        "\n",
        "# run loop for each row\n",
        "# for index, row in server_cross_mixing_df.iterrows():\n",
        "#     # get the neighbourhood id\n",
        "#     neighbourhood_id = row['neighbourhood_id']\n",
        "#     neighbourhood_id -=1 # for 0 based indexing\n",
        "#     # get the crime type id\n",
        "#     crime_type_id = row['TYPE_ID']\n",
        "#     # get the date\n",
        "#     date = row['time']\n",
        "#     # get the time\n",
        "#     time = date.time()\n",
        "#     # get the day\n",
        "#     day = date.date()\n",
        "#     # get the time division\n",
        "#     time_division = int(math.floor(time.hour / hours_per_cell))\n",
        "#     # get the day index\n",
        "#     day_index = (day - START_DATE.date()).days\n",
        "#     # get the time division index\n",
        "#     time_division_index = day_index * time_division_per_day + time_division\n",
        "#     # increment the value in the tensor\n",
        "#     server_crime_tensor[neighbourhood_id][crime_type_id][time_division_index] += 1\n",
        "#     cor_side_id = side_neighbourhood_dict[neighbourhood_id]\n",
        "#     server_side_crime_tensor[cor_side_id][crime_type_id][time_division_index] += 1\n",
        "\n",
        "\n",
        "    # Iterate over the first dimension (total meighbors) and save each 2D slice as a text file\n",
        "output_directory = \"./toy_data/chicago/aggregated\"\n",
        "for i in range(total_meighbors):\n",
        "    filename = f\"{output_directory}/r_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "\n",
        "# now same for side data\n",
        "for i in range(9):\n",
        "    filename = f\"{output_directory}/s_{i}.txt\"\n",
        "    # Extract the 2D slice at index i\n",
        "    slice_2d = server_side_crime_tensor[i, :, :]\n",
        "\n",
        "    # Save the 2D slice as a text file\n",
        "    np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")\n",
        "\n",
        "print('here aggregated data generation ended')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxKVGC2Rh3PT"
      },
      "source": [
        "# Unused"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_hDuE4HosXu"
      },
      "outputs": [],
      "source": [
        "# taxi_trip_df = pd.read_csv('/data/chicago/out_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# # print the shape\n",
        "# print(taxi_trip_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2VPISbhosXu"
      },
      "outputs": [],
      "source": [
        "# # here taxi data set creation code\n",
        "# # this calc is for \"taxi in data\"\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# new_start_date = datetime.strptime('2013-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# new_end_date =  datetime.strptime('2013-09-16 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# # read the Taxi trip csv\n",
        "# # ----------------------------------------\n",
        "# taxi_trip_df = pd.read_csv('/data/chicago/out_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# #-------------------------------------------\n",
        "# # filter the df to have data from new start date to end date\n",
        "# taxi_trip_df = taxi_trip_df[(taxi_trip_df['Trip Start Timestamp'] >= new_start_date) & (taxi_trip_df['Trip Start Timestamp'] <= new_end_date)]\n",
        "# # rename the start time stamp column to start time and end timestamp column to end time, Pickup Community Area to out , Dropoff Community Area to in\n",
        "# taxi_trip_df.rename(columns={'Trip Start Timestamp':'out_time', 'Pickup Community Area':'out'}, inplace=True)\n",
        "# # get the number of days\n",
        "# num_days = (new_end_date - new_start_date).days + 1 # 260\n",
        "# # get the number of time divisions\n",
        "# num_time_divisions = 2  # here time divison is 2 means each day has 2 time slots\n",
        "# # get the number of regions\n",
        "# num_regions = 77\n",
        "# # get the number of rows\n",
        "# num_rows = 2\n",
        "# # get the number of columns\n",
        "# num_cols = num_days * num_time_divisions # 260 * 2\n",
        "# # create a 3d tensor of dimension num_regions X num_rows X num_cols\n",
        "# taxi_trip_tensor = np.zeros((num_regions, num_rows, num_cols))\n",
        "# # Convert the time format to datetime\n",
        "# taxi_trip_df['out_time'] = pd.to_datetime(taxi_trip_df['out_time'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "# # run loop for each row\n",
        "# for index, row in taxi_trip_df.iterrows():\n",
        "#     # check if out and in times are within range\n",
        "#     if row['out_time'] >= new_start_date and row['out_time'] <= new_end_date:\n",
        "#         # get the out region\n",
        "#         out_region = row['out']\n",
        "#         # get the out time\n",
        "#         out_time = row['out_time']\n",
        "#         # get the out day\n",
        "#         out_day = out_time.date()\n",
        "#         # get the out time division\n",
        "#         out_time_division = int(math.floor(out_time.hour / 12))  # here each day is divided in 12 hrs time slot,\n",
        "\n",
        "#         # get the out day index\n",
        "#         out_day_index = (out_day - new_start_date.date()).days\n",
        "#         # get the out time division index\n",
        "#         out_time_division_index = out_day_index * num_time_divisions + out_time_division\n",
        "#         # get the in time division index\n",
        "#         # increment the value in the tensor\n",
        "#         taxi_trip_tensor[out_region - 1][1][out_time_division_index] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuN1KeXlosXv"
      },
      "outputs": [],
      "source": [
        "# # here taxi data set creation code\n",
        "# # this calc is for \"taxi in data\"\n",
        "# from datetime import datetime, timedelta\n",
        "\n",
        "# new_start_date = datetime.strptime('2013-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
        "# new_end_date =  datetime.strptime('2013-09-16 23:59:59', '%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# # read the Taxi trip csv\n",
        "# # ----------------------------------------\n",
        "# taxi_trip_df = pd.read_csv('/data/chicago/in_Data.csv')\n",
        "# # drop the empty rows\n",
        "# taxi_trip_df = taxi_trip_df.dropna()\n",
        "# #-------------------------------------------\n",
        "# # filter the df to have data from new start date to end date\n",
        "# taxi_trip_df = taxi_trip_df[(taxi_trip_df['Trip End Timestamp'] >= new_start_date) & (taxi_trip_df['Trip End Timestamp'] <= new_end_date)]\n",
        "# # rename the start time stamp column to start time and end timestamp column to end time, Pickup Community Area to out , Dropoff Community Area to in\n",
        "# taxi_trip_df.rename(columns={'Trip End Timestamp':'in_time', 'Dropoff Community Area':'in'}, inplace=True)\n",
        "\n",
        "# # Convert the time format to datetime\n",
        "# taxi_trip_df['in_time'] = pd.to_datetime(taxi_trip_df['in_time'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "# # run loop for each row\n",
        "# for index, row in taxi_trip_df.iterrows():\n",
        "#     # check if in and in times are within range\n",
        "#     if row['in_time'] >= new_start_date and row['in_time'] <= new_end_date:\n",
        "#         # get the in region\n",
        "#         in_region = row['in']\n",
        "#         # get the in time\n",
        "#         in_time = row['in_time']\n",
        "#         # get the in day\n",
        "#         in_day = in_time.date()\n",
        "#         # get the in time division\n",
        "#         in_time_division = int(math.floor(in_time.hour / 12)) # here dividing the day in 12 hrs, so each day has two slots indexed 0,1\n",
        "\n",
        "#         # get the in day index\n",
        "#         in_day_index = (in_day - new_start_date.date()).days\n",
        "#         # get the in time division index\n",
        "#         in_time_division_index = in_day_index * num_time_divisions + in_time_division\n",
        "#         # get the in time division index\n",
        "#         # increment the value in the tensor\n",
        "#         taxi_trip_tensor[in_region - 1][0][in_time_division_index] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltjsRwU9osXv"
      },
      "outputs": [],
      "source": [
        "# # for i in range(77):\n",
        "# #     file = open('content/drive/MyDrive/Undergrad_Thesis_Group_2_FedCrime/AIST/data/chicago2/act_ext' + '/taxi_' + str(i+1) + '.txt', 'w')\n",
        "# #     for j in range(total_rows):\n",
        "# #         for k in range(total_cols):\n",
        "# #             file.write(str(int(server_side_crime_tensor[i][j][k])) + \" \")\n",
        "# #         file.write(\"\\n\")\n",
        "# #     file.close()\n",
        "\n",
        "# output_directory = \"/data/chicago/act_ext\"\n",
        "# for i in range(77):\n",
        "#     filename = f\"{output_directory}/taxi_{i+1}.txt\"\n",
        "#     # Extract the 2D slice at index i\n",
        "#     slice_2d = taxi_trip_tensor[i, :, :]\n",
        "\n",
        "#     # Save the 2D slice as a text file\n",
        "#     np.savetxt(filename, slice_2d, fmt=\"%d\", delimiter=\" \")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fedcrime",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
